\documentclass[12pt]{ucsddissertation}
% mathptmx is a Times Roman look-alike (don't use the times package)
% It isn't clear if Times is required. The OGS manual lists several
% "standard fonts" but never says they need to be used.
\usepackage{mathptmx}
\usepackage[NoDate]{currvita}
\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{microtype}
\usepackage[breaklinks=true,pdfborder={0 0 0}]{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{enumitem}
\AtBeginDocument{%
	\settowidth\cvlabelwidth{\cvlabelfont 0000--0000}%
}

%commands

\newcommand{\sword}{SwordBox}
\newcommand{\thesistitle}{Disaggregated Data Structures: Sharing and contention with RDMA and Programmable Networks}

% OGS recommends increasing the margins slightly.
\increasemargins{.1in}

% These are just for testing/examples, delete them
\usepackage{trace}
%\usepackage{showframe} % This package was just to see page margins
\usepackage[english]{babel}
\usepackage{blindtext}
\input{annotations}
\overfullrule5pt
% ---

% Required information
\title{\thesistitle}
\author{Stewart Steven Grant}
\degree{Computer Science}{Doctor of Philosopy}
% Each member of the committee should be listed as Professor Foo Bar.
% If Professor is not the correct title for one, then titles should be
% omitted entirely.
\chair{Alex C. Snoeren}
% Your committee members (other than the chairs) must be in alphabetical order
\committee{Amy Ousterhout}
\committee{Yiying Zhang}
\committee{George Papen}
\degreeyear{2024}

% Start the document
\begin{document}
% Begin with frontmatter and so forth
\frontmatter
\maketitle
\makecopyright
\makesignature
% Optional
\begin{dedication}
\setsinglespacing
\raggedright % It would be better to use \RaggedRight from ragged2e
\parindent0pt\parskip\baselineskip

\textit{To my inner circle -- Family, Loved ones and Friends. And to every belayer that caught me. Thanks for the proverbial and literal support.}


% In recognition of reading this manual before beginning to format the
% doctoral dissertation or master's thesis; for following the
% instructions written herein; for consulting with OGS Academic Affairs
% Advisers; and for not relying on other completed manuscripts, this
% manual is dedicated to all graduate students about to complete the
% doctoral dissertation or master's thesis.

% In recognition that this is my one chance to use whichever
% justification, spacing, writing style, text size, and/or textfont that
% I want to while still keeping my headings and margins consistent.
\end{dedication}
% Optional
\begin{epigraph}
\vskip0pt plus.5fil
\setsinglespacing
{\flushright

"If this is the best of possible worlds, what then are the others?"\\
\vskip\baselineskip
-Voltaire \textit{Candide}\par}

\vfil
\begin{center}

\noindent “It must be considered that there is nothing more difficult to carry out, nor more
doubtful of success, nor more dangerous to handle, than to initiate a new order of things.”

\vskip\baselineskip
\hskip0pt plus1fil -Niccolo Machiavelli \textit{The Prince}\hskip0pt plus4fil\null

\end{center}
\vfil
"If every porkchop were perfect we wouldn't have hotdogs"\\

\vskip\baselineskip
-Greg Universe, \textit{Steven Universe}

\vfil
\end{epigraph}

% Next comes the table of contents, list of figures, list of tables,
% etc. If you have code listings, you can use \listoflistings (or
% \lstlistoflistings) to have it be produced here as well. Same with
% \listofalgorithms.
\tableofcontents
\listoffigures
\listoftables

% Preface
\begin{preface}
\todo{Write a preface - take a look at some examples because it seems rather free form}
Almost nothing is said in the manual about the preface. There is no
indication about how it is to be typeset. Given that, one is forced to
simply typeset it and hope it is accepted. It is, however, optional
and may be omitted.
\end{preface}

% Your fancy acks here. Keep in mind you need to ack each paper you
% use. See the examples here. In addition, each chapter ack needs to
% be repeated at the end of the relevant chapter.
\begin{acknowledgements}

I would like to acknowledged my advisor Alex C. Snoeren for his dedication to his craft and guidance
over the past 6 years. No piece of work within this thesis would be possible without your
collaboration. I would also like to thank my committee members Amy Ousterhout, Yiying Zhang, and
George Papen for their feedback and guidance, and Srikanth Kandula for his mentorship during my time
at MSR.

This thesis has been extraordinarily influenced by Anil Yelam, my closest collaborator. Thank you
for all the time you spent working on our collaborations, and the hours spent discussing and
debating system designs and performance results. I'm forever grateful. To Maxwell Bland, your
research energy is unmatched and without your help we would never have have acquired any SmartNICs.
And Alex (Enze) Liu for his superior knowledge of Python and unmatched focus on research.  Thank you
to all of the members of the Systems and Networking group at UCSD, especially the optical networking
group for your feedback and guidance during the first years of my PhD.

Thank you to Meta for funding my research and providing me with the opportunity to work on resource
disaggregation, Cavium for the generous donation of two SmartNICs, and to ARPAe for funding my first
years of research.

I'd like to thank all of the members of 3140 for their collaboration and friendship over the years.
It's truly the best office, Chez bob volunteers for keeping me fed, and to my friends for the
support. Camille Rubel thanks for having the best climbing schedule in the world, Phillip Arndt for
pushing my limits, and Camille Moore for keeping me on my toes.
\todo{double check I thanked everyone}


\end{acknowledgements}

% Stupid vita goes next
\begin{vita}
\noindent
\begin{cv}{}
\begin{cvlist}{}
\item[2012-2016] Bachelor of Science, Computer Science University of British Columbia
\item[2016-2018] Master of Science, Computer Science University of British Columbia
\item[2018-2024] Doctor of Philosophy, Computer Science University of California, San Diego
\end{cvlist}
\end{cv}

% This puts in the PUBLICATIONS header. Note that it appears inside
% the vita environment. It is optional.
\publications

\noindent Deepak Bansal and Gerald DeGrace and Rishabh Tewari and Michal Zygmunt and James Grantham
and Silvano Gai and Mario Baldi and Krishna Doddapaneni and Arun Selvarajan and Arunkumar Arumugam
and Balakrishnan Raman and Avijit Gupta and Sachin Jain and Deven Jagasia and Evan Langlais and
Pranjal Srivastava and Rishiraj Hazarika and Neeraj Motwani and Soumya Tiwari and Stewart Grant and
Ranveer Chandra and Srikanth Kandula. 2023. Disaggregating Stateful Network Functions. In
proceedings of 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23).
Usenix Association, Boston MA, USA, 1469--1487.
https://www.usenix.org/conference/nsdi23/presentation/bansal \\

\noindent Stewart Grant, Anil Yelam, Maxwell Bland, and Alex C. Snoeren. 2020. SmartNIC Performance
Isolation with FairNIC: Programmable Networking for the Cloud. In Proceedings of the Annual
conference of the ACM Special Interest Group on Data Communication on the applications,
technologies, architectures, and protocols for computer communication (SIGCOMM '20). Association for
Computing Machinery, New York, NY, USA, 681–693. https://doi.org/10.1145/3387514.3405895 \\

\noindent Stewart Grant, Hendrik Cech, and Ivan Beschastnikh. 2018. Inferring and asserting
distributed system invariants. In Proceedings of the 40th International Conference on Software
Engineering (ICSE '18). Association for Computing Machinery, New York, NY, USA, 1149–1159.
https://doi.org/10.1145/3180155.3180199 \\


% This puts in the FIELDS OF STUDY. Also inside vita and also
% optional.
% \fieldsofstudy
% \noindent Major Field: Computer Science
\end{vita}

% Put your maximum 350 word abstract here.
\begin{dissertationabstract} 
%%	
Resource disaggregation proposes a next-generation architecture for data center resources. System
components like compute, memory, storage, and accelerators are separated from one another by a fast
network and composed dynamically into virtual servers when required. This paradigm promises
dramatically improved resource utilization, scalability, and flexibility, but introduces dramatic
challenges in terms of performance and fault tolerance. Memory is among the most difficult resources
to disaggregate. CPUs currently expect DRAM to ultra low latency, high bandwidth, and to share it's
failure domain. In particular increased latency from network round trips dramatically shifts the
performance of existing shared data structures designed for local DRAM.

In this thesis I explore the challenges of sharing disaggregated memory with hundreds of CPU cores.
First in {\sword} I present a system which utilizes a centralized programmable switch to cache data
structure state and dramatically improve key-value workload performance. Second I present a new
key-value store RCuckoo which is designed to leverage RDMA and reduce round trips when accessed by
CPU's over a network. Both systems demonstrate significant performance improvements over the
existing state of the art.

\end{dissertationabstract}

% This is where the main body of your dissertation goes!
\mainmatter

% Optional Introduction
\begin{dissertationintroduction}

\textit{What should a data center server look like?} 20 years ago a data center operator may have
argued for the simplicity of homogeneity over optimal performance, reasoning that carefully picked
hardware at a given price point would yield the best cost-performance tradeoffs and that the
performance yields of next generation hardware would quickly erase any gains made by specializing
servers. 
%%
Since then both Moors law and Dennard scaling have slowed down dramatically. CPU clock speeds and
memory density improvements have stagnated leaving operators to fight tooth and claw to enjoy the
efficiency gains of prior decades. The effect is that new technologies are being introduced to to
achieve scaling.  CPU's are now monstrously parallel. custom accelerators are common for common
workloads like video coding, and machine learning. Indeed the modern datacenter is a hodgepodge of
heterogeneous hardware (GPU's, TPU's, DPU's, SmartNICs and FPGA's)~\cite{nitro}, and various new
memory offerings and tiers like NVMe. Today the same operator that argued for homogeneity would
likely be unable to name every kind of server in their data center. At the time of writing EC2 has
84 listed instance types~\cite{ec2-offer} for their customers to design their services. The trend is
clear, in search of efficiency data center and server design is increasingly heterogeneous with
servers being designed for specific applications and workloads.

Resource disaggregation is a new architectural paradigm for data center resources aimed at improving
efficiency and managing increased heterogeneity. In the disaggregated model a servers resources do
not monolithic reside in a 1, 2, or 4U server form factor, instead each resource i.e compute,
memory, storage is dedicated to a single machine and interconnected via a fast network. Servers are
composed dynamically from these resources which enables them to be provisioned for their exact
purpose. This model enables resource pooling and sharing which in turn leads to higher efficiency.

% DRAM in particular has quickly become a precious resource in the data center. Main memory capacity,
% while growing has not kept up with CPU core counts. The result is that per-core memory capacity is
% less than 10 years ago~\cite{micron-memorywall}. New memory tiers like NVMe have been introduced to
% help alleviate memory pressure from applications, but the composition of these memory tiers, and the
% software to manage them are still undecided.

DRAM in particular has become a precious resource in data centers and is a focused target for
resource pooling~\cite{micron-memorywall}. The benefits of pooling are clear when examining a
simple bin packing problem. Consider 2 servers both provisioned with 4GB of DRAM and 3 jobs each of
which requires 2.5GB of memory. In a monolithic design a scheduler can only place one job per
machine or risk swapping to disk. In a disaggregated model the 4GB of memory would be placed in an
8GB pool which could easily subdivided into 3 2.5GB partitions. In the monolithic case the unused
memory is stranded, pooling reclaims stranded memory.  More concretely at datacenter levels
practitioners have to provision their servers for the sum of peak demand, when resources are pooled
they can be provisioned for the peak of the sum of demand which can be significantly
lower~\cite{supernic, dsnf}.

Disaggregation is only possible because of new fast and programmable networks. Commodity NICs now
offer 400Gbps with expectations for continued growth, and network devices are increasingly
programmable with multiple vendors offering programable SmartNICs, DPUs and switches.  These two
trends have lead to intra rack latencies of 1-2us and network bandwidths on the same order as memory
bandwidths with the ability to inspect, cache, and modify packets in flight at line rate.
%%
But if disaggregation were easy it would have been done already, \textit{and it some cases it has}.
Storage, has seen widespread disaggregation. The key enabling factor is that storage device access
latency is far higher than a network round trip. In the case of memory the opposite is true. Memory
access latency is 20x lower than an RTT effectively making it a separate tier of memory when placed
across a network.  This performance gap plays out in dramatic fashion when porting data structures
designed for local memory to far memory systems. Sharing data between multiple remote machines where
coherence between remote cores is required leads to abysmal performance degradation (300x tail
latencies in some cases). Further complicating the design of shared data structures is that memory,
once in a single fault domain, can fail ripping a hole directly into a processes address space.

This thesis explores the design of shared data structures in disaggregated memory systems.  I
present two systems ({\sword} and RCuckoo) which focus on the challenges of sharing and contending
access to remote memory. {\sword} (Chapter~\ref{chap:swordbox}) is a middlebox approach for removing
contention from shared data structures. The high level insight behind {\sword} is to leverage the
fact that at rack-scale TORs have a serialized view of traffic to and from servers. Using this idea
{\sword} caches data structure state on a programmable switch. 
%%
Second I present RCuckoo (Chapter~\ref{chap:rcuckoo}) a fully disaggregated key-value store. RCuckoo
is specially designed to improve key-value locality. Using locality sensitive hashing RCuckoo
improves, reads, writes, locking and fault recovery performance buy reducing the overall number of
round trips required for each operation.
%%
{\sword} yields orders of magnitude improvements in both
throughput (up to 30x) and tail latency (up to 300x).  RCuckoo meets or exceeds the performance of
all state of the art disaggregated key-value stores on small key-value pairs and significantly
outperforms other systems on read-mostly workloads.

\end{dissertationintroduction}

\chapter{Background}

Calls for distributed memory systems have been made for decades, and the groundwork of
disaggregation relies heavily on the work of the past~\cite{treadmarks,gms}. However, disaggregation
differs from prior distributed systems due to it's scale, and system assumptions. Disaggregation
assumes that each resource be stand alone, and paired with miniscule or no compute power, save for
resources like CPUs an GPUs which are inherently compute.  This assumption transforms the
technologies and requirements for disaggregated systems. Critically Remote Direct Memory Access
(RDMA) (or a similar CPU bypass technology) is essential in constructing a passive storage system.

\section{Disaggregation}

As with many paradigm shifts within the systems community the idea of disaggregation is born from
shifting hardware trends. Per core access to memory has been decreasing for decades. CPU core counts
have consistently increased for over a decade while memory speeds and capacity have improved
comparatively slowly~\cite{micron-memorywall}. The result is that CPU cores have less access to
memory now than 10 years ago. Memory is now an increasingly scarce resource and data center
operators are exploring new ways to achieve better memory utilization. Disaggregation is one such
option. Monolithic servers have a fixed amount of RAM per machine which, at a data center level,
leads to a skewed distributed of memory utilization. Some servers are starving for memory while
others have gigabytes to spare. This spare (stranded) memory is one target of disaggregation, it
aims to give each server access to a shared pool of RAM. In general a disaggregated resource, be it
memory, FPGA's, or the network itself, can be provisioned for the peak-of-sums, rather than
monolithic resources which are provisioned for the sum-of-peaks~\cite{clio,supernic,dsnf}.

The biggest barrier to disaggregation is network latency, and more aptly the difficulty of
disaggregating a resource is proportional it's access latency with respect to the additional latency
of the network. Disaggregated storage is a foregone conclusion. Academias and industry have pooled
SSD's and HDD's into shared storage pools for their increased capacity and lower cost. This is
relatively easy in comparison to an accelerator, or memory as the access cost of storage is already
high relative to the network. In concrete terms intra rack RDMA ping latencies are on the order of
1-2 us. HDD latencies are 10-20ms and SSD are often 100's of us. In these cases the network overhead
is a single digit percentage or less~\cite{decible}\todo{go back and get more citations}. In
contrast DRAM latencies are 50-100ns. DRAM over RDMA has a nearly 20x overhead compared to local
access. Despite this overhead RDMA is one of the strongest contenders for a disaggregated transport.
CXL is a burgeoning technology which promises NUMA like latencies for remote memory
access~\cite{cxl}. At the time of writing CXL is not widely available and it's performance and
scalability are not well understood. Regardless of interconnect the principle of access latency
remains the same. This thesis focuses on RDMA although we see the algorithms and data structures
presented here to be largely interconnect independent.


\section{RDMA}

Remote direct memory access (RDMA) is a low latency, high bandwidth network protocol. It achieves
high performance by bypassing multiple overheads common to a traditional networking stack (ex Linux
Sockets). Firstly RDMA is a kernel bypass technology. It allows user space applications to directly
interact with the network interface card (NIC). This removes two distinct overheads, context
switching and data copy to the kernel. Second, RDMA's most distinguishing feature is that it
offloads much of the network stack to the NIC hardware, and bypasses the CPU entirely for data
transfer. As an example, in a traditional networking stack if a process sends a UDP message on an
existing socket the user first marshals the data and then submits it to the kernel via a
\textit{send}. The kernel then copies the data out of user space, sets up the packet header, and
submits the packet to the NIC. In contrast with RDMA the user space application registers a region
of memory with the NIC prior to sending data. If an application wants to transfer the data in that
region to a remote machine it passes a pointer to the data, and the data's size to the NIC along
with the remote machines's address, and the action the NIC should take (read, write, ect). The
application sends by calling a special \textit{rdma send}. This operation is non-blocking and sends
a signal to the NIC which then DMA's the memory out of the processes address space, constructs a
packet on the NIC (including managing transport state) and sends directly to another machine. The
receiving NIC (in the case of a write), issues the DMA to the remote machine's memory without ever
interacting with it's CPU.  The example above is only a high level overview of an RDMA write, the
RDMA protocol is deeply complex. The current infiniband RDMA specification is over 1700
pages~\cite{infiniband-spec}. In this section I will briefly cover the most important aspects of
RDMA for the purposes of this thesis.

\subsection{One-Sided Verbs}

The RDMA specification provides 5 service types (Reliable Connection, Unreliable Connection,
Reliable Datagram, Unreliable Datagram, RAW Datagram). Additionally RDMA provides \textit{Verbs}
actions the NIC can take, which vary by service type. Reliable services provide acknowledgements and
connection services provide sequence numbers, in-order delivery, and retransmission. Service type
determines which verbs are available for use. Reliable connections provide the strongest guarantees
and allow for the use of \textit{one-sided} verbs which implement memory-like semantics. One sided
verbs entirely bypass the CPU of the remote machine and allow a client to directly read, write, and
atomically modify the remote hosts memory. This ability makes RDMA a powerful tool for designing
disaggregated systems, especially memory systems as it allows designers to build with the assumption
that memory is passive i.e no remote CPU exists. This prototyping ability is quintessential for
understanding how future disaggregated systems will perform.

The memory semantics of one sided RDMA are complex. Not all verbs respect ordering even on the same
connection. RDMA provides a variety of configurations to gain ordering both on and across
connections. Queue pairs (QPs) are connection handles for RDMA. Verbs are issued on a QP and QP are
polled for responses. QP's provided in-order delivery, but not necessarily in-order completion. For
instance issuing a read prior to a write may see the results of the write. Across QP no ordering is
guaranteed by default. RDMA provides two atomic operations compare and swap (CAS) and fetch and add
(FAA) for atomic modification of 64 bit regions of remote memory. The RDMA advanced transport
supports masked versions of these operations to allow for high density atomic regions while reducing
contention~\cite{rdma-masked-cas}. These atomic operations are the critical component for building
shared disaggregated systems. They enable the construction of locks and semaphores, as well as
lock-free data structures.

\section{CXL}
\todo{Write a bit about CXL and how it relates to the RDMA memory model}


\section{Programable Networks}

The past decade has seen the rise of programmable network devices. These devices are capable of
executing users code often at line rate. A huge variety of devices exist: SmartNICs~\cite{fairnic},
DPUs, FPGAs, and programmable switches~\cite{p4} from a wide variety of vendors. Often these devices
provide thin operating systems which allow users to develop and deploy code, either C or P4. These
programmable devices are powerful and transformational tools for designing network systems as they
can offer orders of magnitude performance improvements when deployed in the right
context~\cite{when-computer}, such as sequencing where it has show to offer huge benefits for
consensus~\cite{eris, nopaxos}. In this thesis we leverage the power of programmable switches to
exactly this benefit to get fast in-network serialization for RDMA based data structures (Section
~\ref{chap:swordbox}).

Programmable switches differ from other programmable network devices in that they have orders of
magnitude more aggregate bandwidth, specific pipeline architecture, and exclusively operate on P4
programs.
%%
Programmable switches such as Intel Tofino~\cite{tofino2} utilize a pipeline architecture. Switches
typically have an ingress and egress pipeline which is broken into multiple stages. In the case of
Tofino 2 both ingress and egress have 12 stages. Packets pass through one or more stages and are set
for transmission once each stage of the pipeline used by the program has executed. Per stage
computation is limited, usually to a few instructions, such a fetching from a lookup table
processing a conditional, or modifying and storing packet header data. The result is that P4
programs have a constrained programming model. Loops and recursion are not easily supported and
branching is limited. Finally switch memory is highly constrained per pipeline stage. All P4
accessible memory is SRAM of which each switch only has a few megabytes.
%%
Ideal programs for programmable switches provide a high degree of benefit from a small amount of
computation. As an example a network sequencer which gives a unique monotonic sequence number to
each packet is ideal as it requires only a few small amount of storage space, and limited amount of
computation per packet.

\subsection{Programmable Switch Use (and Misuse) Cases}

Memory constraints on programmable network devices, as well as their programming model and
architecture have a large impact on what kinds of systems can and should be built upon
them~\cite{when-computer}. Cache size and pipeline depth have a large impact on what can be
effectively stored on a programable switch. As part of our exploration of {\sword}
(Chapter~\ref{chap:swordbox}) we examined the use of a programmable switch as a cache for the
replicated log, and found that the switch for a variety of subtle reasons was not a good fit for our
cache despite many prior works on in-network caching and key-value
stores~\cite{netcache,netchain,netkv,netlock}.

In the log system all clients must read all log entries for consistency. This read patten leads to a
bandwidth bottleneck on the server hosting the log as the number of clients increases and the size
of the log entry increases.  Figure~\ref{fig:bandwidth-model} Illustrates how as the number of
clients and the size of individual log entries increases performance falls over. In our initial
design we debated how to deal with the bottleneck. On one hand we could replicate the log across
multiple memory servers and alleviate the broadcast bottleneck by spreading it across replicas.
Alternatively we could cache the most recent writes on a programmable switch.  Initially we thought
that the switch would be an excellent place for the log cache because we could easily reason about
the access pattern of the log, and each client's link bandwidth would be proportional to how much
they read. But the devil is in the details. Due to RDMA and switch pipeline constraints as described
in the following section we found that the switch was an untenable solution.

\begin{figure}
\centering
\includegraphics[width=0.99\linewidth]{fig/bandwidth-model.pdf}
\caption{Bandwidth model for memory replication. 1MS is a single memory node, while 3MS is 3.}
\label{fig:bandwidth-model}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/switch-cache-model.pdf}
    \caption{Performance improvement of caching on a programable switch as a function of hit rate and object cache size}
    \label{fig:switch-cache-model}
\end{figure}

The values in Figure~\ref{fig:switch-cache-model} assume 64 clients and a single memory server and a
log entry size of 8 bytes. The 0\% hit rate on cache sizes of 1024 is almost identical to the
furthest left point in Figure~\ref{fig:scalability}(c) with 64 clients and a single memory node.
Note that with this cache size and a hit rate of 95\% which may be attainable considering the log
access patterns are extremely predictable we are able to handle around 200 Million ops per second,
nearly the same as achieved by a 4 client setup.

Unfortunately current programmable switch hardware does not allow for cashable objects over 192
bytes. Our Tofino 2 switch has a fixed sized 64 bit maximum storage capacity at each stage of it's
pipeline, with a total of 24 stages counting both ingress and egress~\cite{tofino2}. Given a maximum
cache size of 192 we can again calculate our maximum performance improvement. Using max sized RDMA
reads of 192 bytes we significantly drop performance without the use of a switch, and in the common
case get around 68 MOPS, our model without a switch striping across 3 memory servers yields a
maximum of 57 MOPS. While still a theoretical advantage we consider the complexity of adding a
programmable switch and caching for and 18\% benefits to be not worth it, especially considering
that next generation NICs are slated for bandwidths of 800GBPS+. If future switch technology were to
allow for larger object caches we believe this direction may be fruitful.

The purpose of this example is to illustrate the constraints of current programmable switch
hardware. Would a cache be total infeasible on other programable network hardware? Not nessisary,
given 



\section{Disaggregated Systems and Data Structures}

The trends and technologies discussed in the above sections have lead to a new class of system for
disaggregation. These systems are built with the assumption that resources are separated by a fast
network and have little to no heterogeneity. I.e a storage server will be prominently storage which
minimal compute to boot and configure the machine and execute simple management
tasks~\cite{aguilera2019designing,fast-networks,dis-and-app,legoos}.

Disaggregated memory is the focus of many systems. A common model for disaggregated memory is the
same as GMS with the exception that memory is passive and not part of a workstation pool~\cite{gms},
that is remote memory can be used as a swap space or as a remote cache rather than disk. In this
model applications are apportioned a partition of remote memory for it's
pages~\cite{infiniswap,fastswap,leap,blade-server,legoos,hydra}, objects~\cite{aifm,carbink}, or
cache lines~\cite{kona}. These systems focus on improving performance by reducing the number of
remote memory accesses that an application has to make. In general this is done by identifying hot
and cold memory then prefetching and evicting data to reduce the number of faults to remote memory.
Additionally these systems focus on fault-tolerance by replicating or erasure coding memory across
replicated memory servers~\cite{hydra}.

A commonality between each of these memory systems is the lack of sharing. Operations common to
non-disaggregated systems like mmapping a shared page are not supported by these systems. In cases
where shared access is supported using POSIX interfaces performance is not considered to be a
concern~\cite{Regions}. Disaggregated systems that share efficiently, at the time of writing, are
entirely custom built data structures, the majority of which are key-value stores, transaction
processors or caches~\cite{clover, sherman, race, fusee, rolex, ford, ditto}. Each of these system
takes on a significant burden in terms of development. Most develop their own fault tolerance,
replication, recovering, allocation, and serialization protocols. In nearly every case the
performance of these systems is determined by the techniques used to serialize writes on the index
of the data structure.
%%
The techniques used by these systems is largely the same as those used to design RDMA key-value
stores for non-disaggregated memory over the past decade~\cite{pilaf,cell,herd,farm}. The primary
difference is that disaggregated systems use exclusively one sided RDMA operations, while RDMA
key-value stores typically route write operations through a CPU to serialize requests.
%%
A few systems have proposed to or realized the potential of using a programmable switch, or NIC in a
disaggregated rack to aid in serialization. Notably MIND~\cite{mind} implements memory management on
a switch, while Clio~\cite{clio} and SuperNIC~\cite{supernic} use an FPGA based smartNIC to
implement any number of memory side network functions.





\chapter{Swordbox: Accelerated Sharing of Disaggregated Memory}
\label{chap:swordbox}

\input{swordbox/intro}
\input{swordbox/backv2}
\input{swordbox/tmp_sigcomm}
\input{swordbox/implementation}
\input{swordbox/eval}
\input{swordbox/limitations}
\input{swordbox/conclusion}


\chapter{Disaggregated Cuckoo Hashing}
\label{chap:rcuckoo}
\input{rcuckoo/intro}
\input{rcuckoo/background}
\input{rcuckoo/body}
%\input{problems}
\input{rcuckoo/design}
\input{rcuckoo/evaluation}
%\input{limitation}
\input{rcuckoo/conclusion}

% \chapter{Black Box Disaggregation}

% \section{Overview}
% \todo{finish up you writeup for a fall submission of BBD and paste it her}

\chapter{Conclusion}

% \chapter{ Example Figures and such for formatting reference}
% This demonstrates how OGS wants figures and tables formatted. For
% figures, the caption goes below the figure and ``Figure'' is in bold.
% See Figure~\ref{fig:zen}. Tables are formatted with the caption above
% the table. See Table~\ref{tab:bad}.

% Of course, Table~\ref{tab:bad} looks horrible. It should probably be
% formatted like Table~\ref{tab:good} instead.

% For facing caption pages, see Table~\ref{tab:facing}. Of course,
% facing caption pages are vaguely ridiculous and my implementation of
% them in the class file is by far the most brittle part of the
% implementation. It's entirely possible that something has changed and
% these don't work at all. I implemented it merely for the challenge.

% \begin{figure}
% \centering
% \fbox{\parbox{.9\linewidth}{%
% 	\noindent
% 	{\Huge PHD ZEN}\par
% 	\vskip.5in
% 	\centerline{comic here}
% 	\vskip.5in
% }}
% \caption[``Ph.D. Zen'']{Comic entitled ``Ph.D. Zen'' by Jorge Cham, 2005. Copyright
% has not been obtained and so it isn't displayed.}
% \label{fig:zen}
% \end{figure}

% \begin{table}
% \centering
% \caption[Electronic Dissertation Submission Rates]{Electronic
% Dissertation Submission Rates at UCSD, Fall 2005 and Winter 2006.
% (First two quarters that the program was available to all Ph.D.
% candidates not in a Joint Doctoral Program with SDSU.)}
% \label{tab:bad}
% \begin{tabular}{|*{5}{>{\centering\arraybackslash}m{.15\linewidth}|}}
% \hline
% &Ph.D.s awarded (Including Joint degrees) & Electronic submission of
% Dissertation & Paper Submission of Dissertation & Percentage of
% Electronic Submission\\
% \hline
% Fall\par 2005 & 84 & 37 & 47 & 44.05\%\\
% \hline
% Winter\par 2006 & 64 & 42 & 22 & 65.63\%\\
% \hline
% \end{tabular}
% \end{table}

% \begin{table}
% \centering
% \caption[Electronic Dissertation Submission Rates]{Electronic
% Dissertation Submission Rates at UCSD, Fall 2005 and Winter 2006.
% (First two quarters that the program was available to all Ph.D.
% candidates not in a Joint Doctoral Program with SDSU.)}
% \label{tab:good}
% \renewcommand\tabularxcolumn[1]{>{\RaggedRight\arraybackslash}p{#1}}
% \begin{tabularx}{.9\linewidth}{lcccc}
% \toprule
% &\multicolumn{1}{X}{Ph.D.s awarded (Including Joint degrees)}
% &\multicolumn{1}{X}{Electronic submission of Dissertation}
% &\multicolumn{1}{X}{Paper Submission of Dissertation}
% &\multicolumn{1}{X}{Percentage of Electronic Submission}\\
% \midrule
% Fall 2005 & 84 & 37 & 47 & 44.05\%\\
% Winter 2006 & 64 & 42 & 22 & 65.63\%\\
% \bottomrule
% \end{tabularx}
% \end{table}

% \begin{facingcaption}{table}
% \caption[UCSD Gender Distribution]{University of
% California, San Diego Gender Distribution for the Campus Population,
% October~2005\\
% (http://assp.ucsd.edu/analytical/Campus\%20Population.shtml)\\
% \emph{(This is an example of a facing caption page, the next page is
% the example of the table/figure/etc.\ that corresponds to this
% caption. It is also an example of table/figure that is rotated 90
% degrees to fit the page.)}}
% \label{tab:facing}
% \renewcommand\tabularxcolumn[1]{>{\RaggedLeft\arraybackslash}p{#1}}
% \parindent=0pt
% \setbox0=\vbox{%
% \vsize\textwidth
% \hsize\textheight
% \linewidth\hsize
% \columnwidth\hsize
% \textwidth\hsize
% \textheight\vsize
% \begin{tabularx}{\linewidth}{lXXXXXX}
% \toprule
% & \multicolumn{2}{c}{\textbf{Women}}
% & \multicolumn{2}{c}{\textbf{Men}}
% & \multicolumn{2}{c}{\textbf{Total}}\\
% \cmidrule(l){2-3}\cmidrule(l){4-5}\cmidrule(l){6-7}
% \textbf{Population Segment}
% & \multicolumn{1}{c}{\textbf{N}} & \multicolumn{1}{c}{\textbf{\%}}
% & \multicolumn{1}{c}{\textbf{N}} & \multicolumn{1}{c}{\textbf{\%}}
% & \multicolumn{1}{c}{\textbf{N}} & \multicolumn{1}{c}{\textbf{\%}}\\
% \midrule
% Students & 12,987 & 51\% & 12,686 & 49\% & 25,673 & 100\%\\
% Employees & 9,943 & 56\% &  7,671 & 44\% & 17,614 & 100\%\\
% \addlinespace
% \hfill\textbf{Total} & \textbf{22,930} & \textbf{53\%} &
% \textbf{20,357} & \textbf{47\%} & \textbf{43,287} & \textbf{100\%}\\
% \bottomrule
% \end{tabularx}
% \singlespacing

% \emph{Notes}:
% \begin{enumerate}
% \item The counts shown below will differ from the official quarterly
% Registrar's registration report because 1) data for residents in the
% Schools of Medicine and Pharmacy and Pharmaceutical Science are
% excluded, and 2) registered, non-matriculated, visiting students are
% included.
% \item Student workers are excluded from employees; however emeritus
% faculty and others on recall status are included.
% \end{enumerate}

% Campus Planning. Analytical Studies and Space Planning\\
% 31 January 2006
% }
% \centerline{\rotatebox{90}{\box0}}
% \end{facingcaption}

% % This will give us some more text
% % \Blinddocument

% % Skipping a bunch of chapters
% \begin{figure}
% \centering
% \fbox{\hbox to.8\linewidth{\hss Another figure\hss}}
% \caption{Another figure caption}
% \end{figure}
% \begin{table}
% \centering
% \caption{Another table caption}
% \begin{tabular}{ccc}
% \toprule
% X&Y&Z\\
% \midrule
% a&b&c\\
% \bottomrule
% \end{tabular}
% \end{table}
% \begin{figure}
% \caption{ASDF fig}
% \end{figure}
% \begin{table}
% \caption{ASDF tab}
% \end{table}

\appendix
% \Blinddocument
\bibliographystyle{plain} % Or whatever style you want like plainnat

\bibliography{thesis}

% Stuff at the end of the dissertation goes in the back matter
\backmatter

\end{document}
