@article{10.1145/224057.224072,
	title        = {Implementing Global Memory Management in a Workstation Cluster},
	author       = {Feeley, M. J. and Morgan, W. E. and Pighin, E. P. and Karlin, A. R. and Levy, H. M. and Thekkath, C. A.},
	year         = 1995,
	month        = {December},
	journal      = {SIGOPS Oper. Syst. Rev.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 29,
	number       = 5,
	pages        = {201–212},
	doi          = {10.1145/224057.224072},
	issn         = {0163-5980},
	url          = {https://doi.org/10.1145/224057.224072},
	issue_date   = {Dec. 3, 1995},
	numpages     = 12
}
@inproceedings{10.1145/2785956.2787484,
	title        = {Congestion Control for Large-Scale RDMA Deployments},
	author       = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
	year         = 2015,
	booktitle    = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
	location     = {London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '15},
	pages        = {523–536},
	doi          = {10.1145/2785956.2787484},
	isbn         = 9781450335423,
	url          = {https://doi.org/10.1145/2785956.2787484},
	abstract     = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
	keywords     = {ECN, datacenter transport, congestion control, RDMA, PFC},
	numpages     = 14
}
@inproceedings{10.1145/3342195.3387522,
	title        = {Can Far Memory Improve Job Throughput?},
	author       = {Amaro, Emmanuel and Branner-Augmon, Christopher and Luo, Zhihong and Ousterhout, Amy and Aguilera, Marcos K. and Panda, Aurojit and Ratnasamy, Sylvia and Shenker, Scott},
	year         = 2020,
	booktitle    = {Proceedings of the Fifteenth European Conference on Computer Systems},
	location     = {Heraklion, Greece},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {EuroSys ’20},
	doi          = {10.1145/3342195.3387522},
	isbn         = 9781450368827,
	url          = {https://doi.org/10.1145/3342195.3387522},
	articleno    = 14,
	numpages     = 16
}
@inproceedings{254120,
	title        = {Disaggregation and the Application},
	author       = {Sebastian Angel and Mihir Nanavati and Siddhartha Sen},
	year         = 2020,
	month        = {July},
	booktitle    = {12th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
	publisher    = {{USENIX} Association},
	url          = {https://www.usenix.org/conference/hotcloud20/presentation/angel}
}
@misc{3po,
	title        = {3PO: Programmed Far-Memory Prefetching for Oblivious Applications},
	author       = {Christopher Branner-Augmon and Narek Galstyan and Sam Kumar and Emmanuel Amaro and Amy Ousterhout and Aurojit Panda and Sylvia Ratnasamy and Scott Shenker},
	year         = 2022,
	archiveprefix = {arXiv},
	eprint       = {2207.07688},
	primaryclass = {cs.OS}
}


@misc {nitro,
	title = {AWS Nitro System},
	author = {Amazon Web Services},
	howpublished = {https://aws.amazon.com/ec2/nitro/},
	year = {2024}
}

@misc{ec2-offer,
	title = {Amazon EC2 Instances},
	author = {Amazon Web Services},
	howpublished = {https://aws.amazon.com/ec2/instance-types/},
	year={2024}
}

@misc{micron-memorywall,
	title = {Micron’s Perspective on Impact of CXL on DRAM Bit Growth Rate},
	author = {Micron},
	howpublished = {https://www.micron.com/content/dam/micron/global/public/products/white-paper/cxl-impact-dram-bit-growth-white-paper.pdf},
	year={2023},
}

@inproceedings{aguilera2019designing,
	title        = {Designing Far Memory Data Structures: Think Outside the Box},
	author       = {Aguilera, Marcos and Keeton, Kimberly and Novakovic, Stanko and Singhal, Sharad},
	year         = 2019,
	month        = {May},
	booktitle    = {17th Workshop on Hot Topics in Operating Systems (HotOS)},
	url          = {https://www.microsoft.com/en-us/research/publication/designing-far-memory-data-structures-think-outside-the-box/},
	abstract     = {Technologies like RDMA and Gen-Z, which give access to memory outside the box, are gaining in popularity. These technologies provide the abstraction of far memory, where memory is attached to the network and can be accessed by remote processors without mediation by a local processor. Unfortunately, far memory is hard to use because existing data structures are mismatched to it. We argue that we need new data structures for far memory, borrowing techniques from concurrent data structures and distributed systems. We examine the requirements of these data structures and show how to realize them using simple hardware extensions},
	organization = {ACM}
}
@inproceedings{aifm,
	title        = {{AIFM}: High-Performance, Application-Integrated Far Memory},
	author       = {Zhenyuan Ruan and Malte Schwarzkopf and Marcos K. Aguilera and Adam Belay},
	year         = 2020,
	month        = {November},
	booktitle    = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
	publisher    = {{USENIX} Association},
	pages        = {315--332},
	isbn         = {978-1-939133-19-9},
	url          = {https://www.usenix.org/conference/osdi20/presentation/ruan}
}
@inproceedings{amanda-hotnets,
	title        = {Tolerating Faults in Disaggregated Datacenters},
	author       = {Carbonari, Amanda and Beschasnikh, Ivan},
	year         = 2017,
	booktitle    = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
	location     = {Palo Alto, CA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {HotNets-XVI},
	pages        = {164–170},
	doi          = {10.1145/3152434.3152447},
	isbn         = 9781450355698,
	url          = {https://doi.org/10.1145/3152434.3152447},
	abstract     = {Recent research shows that disaggregated datacenters (DDCs) are practical and that DDC resource modularity will benefit both users and operators. This paper explores the implications of disaggregation on application fault tolerance. We expect that resource failures in a DDC will be fine-grained because resources will no longer fate-share. In this context, we look at how DDCs can provide legacy applications with familiar failure semantics and discuss fate sharing granularities that are not available in existing datacenters. We argue that fate sharing and failure mitigation should be programmable, specified by the application, and primarily implemented in the SDN-based network.},
	numpages     = 7
}
@inproceedings{bbn,
	title        = {Black-box Concurrent Data Structures for NUMA Architectures},
	author       = {Calciu, Irina and Sen, Siddhartha and Balakrishnan, Mahesh and Aguilera, Marcos K.},
	year         = 2017,
	booktitle    = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
	location     = {Xi'an, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS '17},
	pages        = {207–221},
	doi          = {10.1145/3037697.3037721},
	isbn         = 9781450344654,
	url          = {https://doi.org/10.1145/3037697.3037721},
	abstract     = {High-performance servers are Non-Uniform Memory Access (NUMA) machines. To fully leverage these machines, programmers need efficient concurrent data structures that are aware of the NUMA performance artifacts. We propose Node Replication (NR), a black-box approach to obtaining such data structures. NR takes an arbitrary sequential data structure and automatically transforms it into a NUMA-aware concurrent data structure satisfying linearizability. Using NR requires no expertise in concurrent data structure design, and the result is free of concurrency bugs. NR draws ideas from two disciplines: shared-memory algorithms and distributed systems. Briefly, NR implements a NUMA-aware shared log, and then uses the log to replicate data structures consistently across NUMA nodes. NR is best suited for contended data structures, where it can outperform lock-free algorithms by 3.1x, and lock-based solutions by 30x. To show the benefits of NR to a real application, we apply NR to the data structures of Redis, an in-memory storage system. The result outperforms other methods by up to 14x. The cost of NR is additional memory for its log and replicas.},
	keywords     = {replication, numa architecture, log, concurrent data structures, black-box techniques},
	numpages     = 15
}
@inproceedings{bedrock,
	title        = {Bedrock: Programmable Network Support for Secure {RDMA} Systems},
	author       = {Jiarong Xing and Kuo-Feng Hsu and Yiming Qiu and Ziyang Yang and Hongyi Liu and Ang Chen},
	year         = 2022,
	month        = {August},
	booktitle    = {31st USENIX Security Symposium (USENIX Security 22)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	pages        = {2585--2600},
	isbn         = {978-1-939133-31-1},
	url          = {https://www.usenix.org/conference/usenixsecurity22/presentation/xing}
}
@inproceedings{blade-server,
	title        = {Disaggregated Memory for Expansion and Sharing in Blade Servers},
	author       = {Lim, Kevin and Chang, Jichuan and Mudge, Trevor and Ranganathan, Parthasarathy and Reinhardt, Steven K. and Wenisch, Thomas F.},
	year         = 2009,
	booktitle    = {Proceedings of the 36th Annual International Symposium on Computer Architecture},
	location     = {Austin, TX, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ISCA '09},
	pages        = {267–278},
	doi          = {10.1145/1555754.1555789},
	isbn         = 9781605585260,
	url          = {https://doi.org/10.1145/1555754.1555789},
	abstract     = {Analysis of technology and application trends reveals a growing imbalance in the peak compute-to-memory-capacity ratio for future servers. At the same time, the fraction contributed by memory systems to total datacenter costs and power consumption during typical usage is increasing. In response to these trends, this paper re-examines traditional compute-memory co-location on a single system and details the design of a new general-purpose architectural building block-a memory blade-that allows memory to be "disaggregated" across a system ensemble. This remote memory blade can be used for memory capacity expansion to improve performance and for sharing memory across servers to reduce provisioning and power costs. We use this memory blade building block to propose two new system architecture solutions-(1) page-swapped remote memory at the virtualization layer, and (2) block-access remote memory with support in the coherence hardware-that enable transparent memory expansion and sharing on commodity-based systems. Using simulations of a mix of enterprise benchmarks supplemented with traces from live datacenters, we demonstrate that memory disaggregation can provide substantial performance benefits (on average 10X) in memory constrained environments, while the sharing enabled by our solutions can improve performance-per-dollar by up to 57% when optimizing memory provisioning across multiple servers.},
	keywords     = {memory capacity expansion, power and cost efficiencies, disaggregated memory, memory blades},
	numpages     = 12
}
@inproceedings{borg,
	title        = {Large-scale cluster management at {Google} with {Borg}},
	author       = {Abhishek Verma and Luis Pedrosa and Madhukar R. Korupolu and David Oppenheimer and Eric Tune and John Wilkes},
	year         = 2015,
	booktitle    = {Proceedings of the European Conference on Computer Systems (EuroSys)},
	address      = {Bordeaux, France}
}
@inproceedings{cachecloud,
	title        = {CacheCloud: Towards Speed-of-light Datacenter Communication},
	author       = {Shelby Thomas and Geoffrey M. Voelker and George Porter},
	year         = 2018,
	booktitle    = {10th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 18)},
	publisher    = {{USENIX} Association},
	address      = {Boston, MA},
	url          = {https://www.usenix.org/conference/hotcloud18/presentation/thomas}
}
@inproceedings{caladan,
	title        = {Caladan: Mitigating Interference at Microsecond Timescales},
	author       = {Joshua Fried and Zhenyuan Ruan and Amy Ousterhout and Adam Belay},
	year         = 2020,
	month        = {November},
	booktitle    = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
	publisher    = {USENIX Association},
	pages        = {281--297},
	isbn         = {978-1-939133-19-9},
	url          = {https://www.usenix.org/conference/osdi20/presentation/fried}
}
@inproceedings{carbink,
	title        = {Carbink: {Fault-Tolerant} Far Memory},
	author       = {Yang Zhou and Hassan M. G. Wassel and Sihang Liu and Jiaqi Gao and James Mickens and Minlan Yu and Chris Kennelly and Paul Turner and David E. Culler and Henry M. Levy and Amin Vahdat},
	year         = 2022,
	month        = {July},
	booktitle    = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
	publisher    = {USENIX Association},
	address      = {Carlsbad, CA},
	pages        = {55--71},
	isbn         = {978-1-939133-28-1},
	url          = {https://www.usenix.org/conference/osdi22/presentation/zhou-yang}
}
@misc{cavium,
	title        = {Liquid {IO II} 10/25G Smart {NIC} Family},
	author       = {Cavium},
	year         = 2017,
	howpublished = {https://www.marvell.com/documents/08icqisgkbtn6kstgzh4/}
}
@inproceedings{cell,
	title        = {Balancing {CPU} and Network in the Cell Distributed {B-Tree} Store},
	author       = {Christopher Mitchell and Kate Montgomery and Lamont Nelson and Siddhartha Sen and Jinyang Li},
	year         = 2016,
	month        = {June},
	booktitle    = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
	publisher    = {{USENIX} Association},
	address      = {Denver, CO},
	pages        = {451--464},
	isbn         = {978-1-931971-30-0},
	url          = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/mitchell}
}
@inproceedings{clio,
	title        = {Clio: A Hardware-Software Co-Designed Disaggregated Memory System},
	author       = {Guo, Zhiyuan and Shan, Yizhou and Luo, Xuhao and Huang, Yutong and Zhang, Yiying},
	year         = 2022,
	booktitle    = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	location     = {Lausanne, Switzerland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS '22},
	pages        = {417–433},
	doi          = {10.1145/3503222.3507762},
	isbn         = 9781450392051,
	url          = {https://doi.org/10.1145/3503222.3507762},
	abstract     = {Memory disaggregation has attracted great attention recently because of its benefits in efficient memory utilization and ease of management. So far, memory disaggregation research has all taken one of two approaches: building/emulating memory nodes using regular servers or building them using raw memory devices with no processing power. The former incurs higher monetary cost and faces tail latency and scalability limitations, while the latter introduces performance, security, and management problems. Server-based memory nodes and memory nodes with no processing power are two extreme approaches. We seek a sweet spot in the middle by proposing a hardware-based memory disaggregation solution that has the right amount of processing power at memory nodes. Furthermore, we take a clean-slate approach by starting from the requirements of memory disaggregation and designing a memory-disaggregation-native system. We built Clio, a disaggregated memory system that virtualizes, protects, and manages disaggregated memory at hardware-based memory nodes. The Clio hardware includes a new virtual memory system, a customized network system, and a framework for computation offloading. In building Clio, we not only co-design OS functionalities, hardware architecture, and the network system, but also co-design compute nodes and memory nodes. Our FPGA prototype of Clio demonstrates that each memory node can achieve 100&nbsp;Gbps throughput and an end-to-end latency of 2.5&nbsp;µ s at median and 3.2&nbsp;µ s at the 99th percentile. Clio also scales much better and has orders of magnitude lower tail latency than RDMA. It has 1.1\texttimes{} to 3.4\texttimes{} energy saving compared to CPU-based and SmartNIC-based disaggregated memory systems and is 2.7\texttimes{} faster than software-based SmartNIC solutions.},
	keywords     = {FPGA, Resource Disaggregation, Virtual Memory, Hardware- Software Co-design},
	numpages     = 17
}
@misc{clio-arxiv,
	title        = {Clio: A Hardware-Software Co-Designed Disaggregated Memory System},
	author       = {Zhiyuan Guo and Yizhou Shan and Xuhao Luo and Yutong Huang and Yiying Zhang},
	year         = 2021,
	archiveprefix = {arXiv},
	eprint       = {2108.03492},
	primaryclass = {cs.DC}
}
@inproceedings{cliquemap,
	title        = {CliqueMap: Productionizing an {RMA-Based} Distributed Caching System},
	author       = {Singhvi, Arjun and Akella, Aditya and Anderson, Maggie and Cauble, Rob and Deshmukh, Harshad and Gibson, Dan and Martin, Milo M. K. and Strominger, Amanda and Wenisch, Thomas F. and Vahdat, Amin},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	series       = {SIGCOMM '21},
	pages        = {93–105},
	numpages     = 13
}
@inproceedings{clover,
	title        = {Disaggregating Persistent Memory and Controlling Them Remotely: An Exploration of Passive Disaggregated Key-Value Stores},
	author       = {Shin-Yeh Tsai and Yizhou Shan and Yiying Zhang},
	year         = 2020,
	month        = {July},
	booktitle    = {{USENIX} Annual Technical Conference},
	pages        = {33--48},
	isbn         = {978-1-939133-14-4},
	url          = {https://www.usenix.org/conference/atc20/presentation/tsai}
}
@misc{connectx,
	title        = {{ConnectX} {SmartNICs}},
	author       = {{NVIDIA}},
	howpublished = {https://www.nvidia.com/en-us/networking/ethernet-adapters/}
}
@inproceedings{corundum,
	title        = {Corundum: An Open-Source {100-Gbps} {NIC}},
	author       = {Alex {Forencich} and Alex C. {Snoeren} and George {Porter} and George {Papen}},
	year         = 2020,
	booktitle    = {2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	pages        = {38--46},
	notes        = {Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3034977853}
}
@article{cuckoo,
	title        = {Cuckoo hashing},
	author       = {Rasmus Pagh and Flemming Friche Rodler},
	year         = 2004,
	journal      = {Journal of Algorithms},
	volume       = 51,
	number       = 2,
	pages        = {122--144},
	doi          = {https://doi.org/10.1016/j.jalgor.2003.12.002},
	issn         = {0196-6774},
	url          = {https://www.sciencedirect.com/science/article/pii/S0196677403001925},
	abstract     = {We present a simple dictionary with worst case constant lookup time, equaling the theoretical performance of the classic dynamic perfect hashing scheme of Dietzfelbinger et al. [SIAM J. Comput. 23 (4) (1994) 738–761]. The space usage is similar to that of binary search trees. Besides being conceptually much simpler than previous dynamic dictionaries with worst case constant lookup time, our data structure is interesting in that it does not use perfect hashing, but rather a variant of open addressing where keys can be moved back in their probe sequences. An implementation inspired by our algorithm, but using weaker hash functions, is found to be quite practical. It is competitive with the best known dictionaries having an average case (but no nontrivial worst case) guarantee on lookup time.},
	keywords     = {Data structures, Dictionaries, Information retrieval, Searching, Hashing, Experiments}
}
@inproceedings{cuckoo-improvements,
	title        = {Algorithmic Improvements for Fast Concurrent Cuckoo Hashing},
	author       = {Li, Xiaozhou and Andersen, David G. and Kaminsky, Michael and Freedman, Michael J.},
	year         = 2014,
	booktitle    = {Proceedings of the Ninth European Conference on Computer Systems},
	location     = {Amsterdam, The Netherlands},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {EuroSys '14},
	doi          = {10.1145/2592798.2592820},
	isbn         = 9781450327046,
	url          = {https://doi.org/10.1145/2592798.2592820},
	abstract     = {Fast concurrent hash tables are an increasingly important building block as we scale systems to greater numbers of cores and threads. This paper presents the design, implementation, and evaluation of a high-throughput and memory-efficient concurrent hash table that supports multiple readers and writers. The design arises from careful attention to systems-level optimizations such as minimizing critical section length and reducing interprocessor coherence traffic through algorithm re-engineering. As part of the architectural basis for this engineering, we include a discussion of our experience and results adopting Intel's recent hardware transactional memory (HTM) support to this critical building block. We find that naively allowing concurrent access using a coarse-grained lock on existing data structures reduces overall performance with more threads. While HTM mitigates this slowdown somewhat, it does not eliminate it. Algorithmic optimizations that benefit both HTM and designs for fine-grained locking are needed to achieve high performance.Our performance results demonstrate that our new hash table design---based around optimistic cuckoo hashing---outperforms other optimized concurrent hash tables by up to 2.5x for write-heavy workloads, even while using substantially less memory for small key-value items. On a 16-core machine, our hash table executes almost 40 million insert and more than 70 million lookup operations per second.},
	articleno    = 27,
	numpages     = 14
}
@misc{cx7,
	title        = {ConnectX-7 Ethernet Datasheet},
	author       = {Nvida},
	year         = 2021,
	url          = {https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectx-7-datasheet-Final.pdf}
}
@misc{cx8,
	title        = {NVIDIA Quantum-X800 Infiniband Platform},
	author       = {Nvidia},
	year         = 2024,
	url          = {https://nvdam.widen.net/s/hbp8zz7fvt/solution-overview-gtcspring24-quantum-x800-3175164}
}
@misc{cxl,
	title        = {{CXL} 3.0 Specification},
	author       = {{CXL Consortium}},
	howpublished = {https://www.computeexpresslink.org/download-the-specification}
}
@article{datacenter-workloads,
	title        = {Workload analysis of a large-scale key-value store},
	author       = {Atikoglu, Berk and Xu, Yuehai and Frachtenberg, Eitan and Jiang, Song and Paleczny, Mike},
	year         = 2012,
	month        = {06},
	journal      = {Sigmetrics Performance Evaluation Review - SIGMETRICS},
	volume       = 40,
	pages        = {},
	doi          = {10.1145/2318857.2254766}
}
@article{dcqcn,
	title        = {Congestion Control for Large-Scale {RDMA} Deployments},
	author       = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
	year         = 2015,
	month        = {August},
	journal      = {SIGCOMM Comput. Commun. Rev.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 45,
	number       = 4,
	pages        = {523–536},
	doi          = {10.1145/2829988.2787484},
	issn         = {0146-4833},
	url          = {https://doi.org/10.1145/2829988.2787484},
	abstract     = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
	issue_date   = {October 2015},
	keywords     = {PFC, datacenter transport, RDMA, ECN, congestion control},
	numpages     = 14
}
@inproceedings{decible,
	title        = {Decibel: Isolation and Sharing in Disaggregated Rack-Scale Storage},
	author       = {Mihir Nanavati and Jake Wires and Andrew Warfield},
	year         = 2017,
	month        = {March},
	booktitle    = {14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17)},
	publisher    = {{USENIX} Association},
	address      = {Boston, MA},
	pages        = {17--33},
	isbn         = {978-1-931971-37-9},
	url          = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/nanavati}
}
@inproceedings{design-guidelines,
	title        = {Design Guidelines for High Performance {RDMA} Systems},
	author       = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
	year         = 2016,
	month        = {June},
	booktitle    = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
	publisher    = {{USENIX} Association},
	address      = {Denver, CO},
	pages        = {437--450},
	isbn         = {978-1-931971-30-0},
	url          = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia}
}
@misc{device-memory,
	title        = {Device memory programming},
	author       = {NVIDIA},
	howpublished = {https://docs.nvidia.com/networking/display/ OFEDv502180/Programming\#Programming-DeviceMemoryProgramming}
}
@inproceedings{dilos,
	title        = {DiLOS: adding performance to paging-based memory disaggregation},
	author       = {Yoon, Wonsup and Oh, Jinyoung and Ok, Jisu and Moon, Sue and Kwon, Youngjin},
	year         = 2021,
	booktitle    = {Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems},
	location     = {Hong Kong, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {APSys '21},
	pages        = {70–78},
	doi          = {10.1145/3476886.3477507},
	isbn         = 9781450386982,
	url          = {https://doi.org/10.1145/3476886.3477507},
	abstract     = {Memory disaggregation places computing and memory in physically separate nodes and achieves improved memory utilization in datacenters. Kernel-based approaches for memory disaggregation offer transparent virtual memory by using paging schemes but suffer from expensive page fault handling. As an alternative, library-based approaches incorporate application semantics to memory disaggregation and can even eliminate page fault handling on its data path. However, its lack of compatibility harms generality and obstruct wide adoption.This paper revisits the paging-based approaches and challenges their performance. We posit that the page fault overhead is not a fundamental limitation. We propose DiLOS, a new memory disaggregating unikernel, that delivers both performance and generality. The key insight of DiLOS to overcome performance drawbacks while maintaining generality lies in the design of a fast, lightweight page fault handler on top of the unikernel's simple execution model. Since each unikernel serves a single application, it also opens room for extra optimization via app-aware prefetching. DiLOS outperforms a recent library-based system (AIFM) by 1.52\texttimes{} and 1.31\texttimes{} when the cache size is 12.5\% and 100\% of the total working set, respectively. Compared to the state-of-the-art paging-based system (Fastswap), DiLOS with a general-purpose prefetcher achieves up to 2.2\texttimes{} higher performance in real-world workload. An app-aware prefetcher further improves the throughput of Redis in-memory database up to 27\%.},
	keywords     = {unikernel, memory disaggregation, disaggregated data center},
	numpages     = 9
}
@inproceedings{direct-cxl,
	title        = {Direct Access, {High-Performance} Memory Disaggregation with {DirectCXL}},
	author       = {Donghyun Gouk and Sangwon Lee and Miryeong Kwon and Myoungsoo Jung},
	year         = 2022,
	month        = {July},
	booktitle    = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
	publisher    = {USENIX Association},
	address      = {Carlsbad, CA},
	pages        = {287--294},
	isbn         = {978-1-939133-29-65},
	url          = {https://www.usenix.org/conference/atc22/presentation/gouk}
}
@inproceedings{dis-and-app,
	title        = {Disaggregation and the Application},
	author       = {Sebastian Angel and Mihir Nanavati and Siddhartha Sen},
	year         = 2020,
	month        = {July},
	booktitle    = {12th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
	publisher    = {{USENIX} Association},
	url          = {https://www.usenix.org/conference/hotcloud20/presentation/angel}
}
@inproceedings{disandapp,
	title        = {Disaggregation and the Application},
	author       = {Sebastian Angel and Mihir Nanavati and Siddhartha Sen},
	year         = 2020,
	month        = {July},
	booktitle    = {12th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
	publisher    = {{USENIX} Association},
	url          = {https://www.usenix.org/conference/hotcloud20/presentation/angel}
}
@inproceedings{dredbox,
	title        = {dReDBox: Materializing a full-stack rack-scale system prototype of a next-generation disaggregated datacenter},
	author       = {Bielski, M. and Syrigos, I. and Katrinis, K. and Syrivelis, D. and Reale, A. and Theodoropoulos, D. and Alachiotis, N. and Pnevmatikatos, D. and Pap, E.H. and Zervas, G. and Mishra, V. and Saljoghei, A. and Rigo, A. and Zazo, J. Fernando and Lopez-Buedo, S. and Torrents, M. and Zyulkyarov, F. and Enrico, M. and de Dios, O. Gonzalez},
	year         = 2018,
	booktitle    = {2018 Design, Automation   Test in Europe Conference   Exhibition (DATE)},
	volume       = {},
	number       = {},
	pages        = {1093--1098},
	doi          = {10.23919/DATE.2018.8342174}
}
@inproceedings{dsnf,
	title        = {Disaggregating Stateful Network Functions},
	author       = {Deepak Bansal and Gerald DeGrace and Rishabh Tewari and Michal Zygmunt and James Grantham and Silvano Gai and Mario Baldi and Krishna Doddapaneni and Arun Selvarajan and Arunkumar Arumugam and Balakrishnan Raman and Avijit Gupta and Sachin Jain and Deven Jagasia and Evan Langlais and Pranjal Srivastava and Rishiraj Hazarika and Neeraj Motwani and Soumya Tiwari and Stewart Grant and Ranveer Chandra and Srikanth Kandula},
	year         = 2023,
	month        = {April},
	booktitle    = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	pages        = {1469--1487},
	isbn         = {978-1-939133-33-5},
	url          = {https://www.usenix.org/conference/nsdi23/presentation/bansal}
}

@inproceedings{eris,
	title        = {Eris: Coordination-Free Consistent Transactions using Network Multi-Sequencing},
	author       = {Li, Jialin and Michael, Ellis and Ports, Dan R. K.},
	year         = 2017,
	month        = {October},
	booktitle    = {Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP '17)},
	url          = {https://www.microsoft.com/en-us/research/publication/eris-coordination-free-consistent-transactions-using-network-multi-sequencing/},
	edition      = {Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP '17)}
}
@inproceedings{erpc,
	title        = {Datacenter {RPCs} can be General and Fast},
	author       = {Anuj Kalia and Michael Kaminsky and David Andersen},
	year         = 2019,
	month        = {February},
	booktitle    = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
	publisher    = {{USENIX} Association},
	address      = {Boston, MA},
	pages        = {1--16},
	isbn         = {978-1-931971-49-2},
	url          = {https://www.usenix.org/conference/nsdi19/presentation/kalia}
}
@inproceedings{faast,
	title        = {{FaSST}: Fast, Scalable and Simple Distributed Transactions with {Two-Sided} ({{{{{RDMA}}}}}) Datagram {RPCs}},
	author       = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
	year         = 2016,
	month        = {November},
	booktitle    = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
	publisher    = {USENIX Association},
	address      = {Savannah, GA},
	pages        = {185--201},
	isbn         = {978-1-931971-33-1},
	url          = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kalia}
}
@misc{facebook-cxl-tpp,
	month        = {April},
	publisher    = {USENIX Association},
	pages        = {785--808},
	doi          = {10.48550/ARXIV.2206.02878},
	url          = {https://arxiv.org/abs/2206.02878}
}
@inproceedings{facebook-memcached,
	title        = {Scaling Memcache at Facebook},
	author       = {Rajesh Nishtala and Hans Fugal and Steven Grimm and Marc Kwiatkowski and Herman Lee and Harry C. Li and Ryan McElroy and Mike Paleczny and Daniel Peek and Paul Saab and David Stafford and Tony Tung and Venkateshwaran Venkataramani},
	year         = 2013,
	month        = {April},
	booktitle    = {10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13)},
	publisher    = {USENIX Association},
	address      = {Lombard, IL},
	pages        = {385--398},
	isbn         = {978-1-931971-00-3},
	url          = {https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/nishtala}
}
@inproceedings{farm,
	title        = {{FaRM}: Fast Remote Memory},
	author       = {Aleksandar Dragojevi{\'c} and Dushyanth Narayanan and Miguel Castro and Orion Hodson},
	year         = 2014,
	month        = {April},
	booktitle    = {11th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 14)},
	publisher    = {{USENIX} Association},
	address      = {Seattle, WA},
	pages        = {401--414},
	isbn         = {978-1-931971-09-6},
	url          = {https://www.usenix.org/conference/nsdi14/technical-sessions/dragojevi{\'c}}
}
@inproceedings{fasst,
	title        = {{FaSST}: Fast, Scalable and Simple Distributed Transactions with Two-Sided ({RDMA}) Datagram {RPCs}},
	author       = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
	year         = 2016,
	month        = {November},
	booktitle    = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
	publisher    = {{USENIX} Association},
	address      = {Savannah, GA},
	pages        = {185--201},
	isbn         = {978-1-931971-33-1},
	url          = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kalia}
}
@inproceedings{fastswap,
	title        = {Can Far Memory Improve Job Throughput?},
	author       = {Amaro, Emmanuel and Branner-Augmon, Christopher and Luo, Zhihong and Ousterhout, Amy and Aguilera, Marcos K. and Panda, Aurojit and Ratnasamy, Sylvia and Shenker, Scott},
	year         = 2020,
	booktitle    = {Proceedings of the Fifteenth European Conference on Computer Systems},
	location     = {Heraklion, Greece},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {EuroSys '20},
	doi          = {10.1145/3342195.3387522},
	isbn         = 9781450368827,
	url          = {https://doi.org/10.1145/3342195.3387522},
	abstract     = {As memory requirements grow, and advances in memory technology slow, the availability of sufficient main memory is increasingly the bottleneck in large compute clusters. One solution to this is memory disaggregation, where jobs can remotely access memory on other servers, or far memory. This paper first presents faster swapping mechanisms and a far memory-aware cluster scheduler that make it possible to support far memory at rack scale. Then, it examines the conditions under which this use of far memory can increase job throughput. We find that while far memory is not a panacea, for memory-intensive workloads it can provide performance improvements on the order of 10% or more even without changing the total amount of memory available.},
	articleno    = 14,
	numpages     = 16
}
@inproceedings{filemr,
	title        = {{FileMR}: Rethinking {RDMA} Networking for Scalable Persistent Memory},
	author       = {Jian Yang and Joseph Izraelevitz and Steven Swanson},
	year         = 2020,
	month        = {February},
	booktitle    = {Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
	pages        = {111--125}
}
@inproceedings{firebox,
	title        = {{FirePerf}: {FPGA}-Accelerated Full-System Hardware/Software Performance Profiling and Co-Design},
	author       = {Karandikar, Sagar and Ou, Albert and Amid, Alon and Mao, Howard and Katz, Randy and Nikoli\'{c}, Borivoje and Asanovi\'{c}, Krste},
	year         = 2020,
	booktitle    = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
	location     = {Lausanne, Switzerland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS '20},
	pages        = {715–731},
	doi          = {10.1145/3373376.3378455},
	isbn         = 9781450371025,
	url          = {https://doi.org/10.1145/3373376.3378455},
	abstract     = {Achieving high-performance when developing specialized hardware/software systems requires understanding and improving not only core compute kernels, but also intricate and elusive system-level bottlenecks. Profiling these bottlenecks requires both high-fidelity introspection and the ability to run sufficiently many cycles to execute complex software stacks, a challenging combination. In this work, we enable agile full-system performance optimization for hardware/software systems with FirePerf, a set of novel out-of-band system-level performance profiling capabilities integrated into the open-source FireSim FPGA-accelerated hardware simulation platform. Using out-of-band call stack reconstruction and automatic performance counter insertion, FirePerf enables introspecting into hardware and software at appropriate abstraction levels to rapidly identify opportunities for software optimization and hardware specialization, without disrupting end-to-end system behavior like traditional profiling tools. We demonstrate the capabilities of FirePerf with a case study that optimizes the hardware/software stack of an open-source RISC-V SoC with an Ethernet NIC to achieve 8x end-to-end improvement in achievable bandwidth for networking applications running on Linux. We also deploy a RISC-V Linux kernel optimization discovered with FirePerf on commercial RISC-V silicon, resulting in up to 1.72x improvement in network performance.},
	keywords     = {fpga-accelerated simulation, network performance optimization, performance profiling, agile hardware, hardware/software co-design},
	numpages     = 17
}
@inproceedings{firmament,
	title        = {Firmament: Fast, Centralized Cluster Scheduling at Scale},
	author       = {Ionel Gog and Malte Schwarzkopf and Adam Gleave and Robert N. M. Watson and Steven Hand},
	year         = 2016,
	month        = {November},
	booktitle    = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
	publisher    = {USENIX Association},
	address      = {Savannah, GA},
	pages        = {99--115},
	isbn         = {978-1-931971-33-1},
	url          = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gog}
}
@inproceedings{flat-combining,
	title        = {Flat combining and the synchronization-parallelism tradeoff},
	author       = {Hendler, Danny and Incze, Itai and Shavit, Nir and Tzafrir, Moran},
	year         = 2010,
	booktitle    = {Proceedings of the Twenty-Second Annual ACM Symposium on Parallelism in Algorithms and Architectures},
	location     = {Thira, Santorini, Greece},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SPAA '10},
	pages        = {355–364},
	doi          = {10.1145/1810479.1810540},
	isbn         = 9781450300797,
	url          = {https://doi.org/10.1145/1810479.1810540},
	abstract     = {Traditional data structure designs, whether lock-based or lock-free, provide parallelism via fine grained synchronization among threads.We introduce a new synchronization paradigm based on coarse locking, which we call flat combining. The cost of synchronization in flat combining is so low, that having a single thread holding a lock perform the combined access requests of all others, delivers, up to a certain non-negligible concurrency level, better performance than the most effective parallel finely synchronized implementations. We use flat-combining to devise, among other structures, new linearizable stack, queue, and priority queue algorithms that greatly outperform all prior algorithms.},
	keywords     = {concurrent data-structures, multiprocessors, synchronization},
	numpages     = 10
}
@inproceedings{flock,
	title        = {Birds of a Feather Flock Together: Scaling RDMA RPCs with Flock},
	author       = {Monga, Sumit Kumar and Kashyap, Sanidhya and Min, Changwoo},
	year         = 2021,
	booktitle    = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
	location     = {Virtual Event, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOSP '21},
	pages        = {212–227},
	doi          = {10.1145/3477132.3483576},
	isbn         = 9781450387095,
	url          = {https://doi.org/10.1145/3477132.3483576},
	abstract     = {RDMA-capable networks are gaining traction with datacenter deployments due to their high throughput, low latency, CPU efficiency, and advanced features, such as remote memory operations. However, efficiently utilizing RDMA capability in a common setting of high fan-in, fan-out asymmetric network topology is challenging. For instance, using RDMA programming features comes at the cost of connection scalability, which does not scale with increasing cluster size. To address that, several works forgo some RDMA features by only focusing on conventional RPC APIs.In this work, we strive to exploit the full capability of RDMA, while scaling the number of connections regardless of the cluster size. We present Flock, a communication framework for RDMA networks that uses hardware provided reliable connection. Using a partially shared model, Flock departs from the conventional RDMA design by enabling connection sharing among threads, which provides significant performance improvements contrary to the widely held belief that connection sharing deteriorates performance. At its core, Flock uses a connection handle abstraction for connection multiplexing; a new coalescing-based synchronization approach for efficient network utilization; and a load-control mechanism for connections with symbiotic send-recv scheduling, which reduces the synchronization overheads associated with connection sharing along with ensuring fair utilization of network connections. We demonstrate the benefits for a distributed transaction processing system and an in-memory index, where it outperforms other RPC systems by up to 88% and 50%, respectively, with significant reductions in median and tail latency.},
	keywords     = {Network hardware, Remote Memory Access},
	numpages     = 16
}
@inproceedings{ford,
	title        = {{FORD}: Fast One-sided {RDMA-based} Distributed Transactions for Disaggregated Persistent Memory},
	author       = {Ming Zhang and Yu Hua and Pengfei Zuo and Lurong Liu},
	year         = 2022,
	month        = {February},
	booktitle    = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
	publisher    = {USENIX Association},
	address      = {Santa Clara, CA},
	pages        = {51--68},
	isbn         = {978-1-939133-26-7},
	url          = {https://www.usenix.org/conference/fast22/presentation/zhang-ming}
}
@misc{fungible,
	title        = {The {Fungible} Data Processing Unit},
	author       = {Fungible},
	year         = 2021,
	howpublished = {https://www.fungible.com/product/dpu-platform/}
}
@inproceedings{fusee,
	title        = {{FUSEE}: A Fully {Memory-Disaggregated} {Key-Value} Store},
	author       = {Jiacheng Shen and Pengfei Zuo and Xuchuan Luo and Tianyi Yang and Yuxin Su and Yangfan Zhou and Michael R. Lyu},
	year         = 2023,
	month        = {February},
	booktitle    = {21st USENIX Conference on File and Storage Technologies (FAST 23)},
	publisher    = {USENIX Association},
	address      = {Santa Clara, CA},
	pages        = {81--98},
	isbn         = {978-1-939133-32-8},
	url          = {https://www.usenix.org/conference/fast23/presentation/shen}
}
@misc{genz,
	title        = {{Gen-Z Consortium}},
	author       = {GenZ},
	year         = 2018,
	howpublished = {https://genzconsortium.org/}
}
@inproceedings{gms,
	title        = {Implementing Global Memory Management in a Workstation Cluster},
	author       = {Feeley, M. J. and Morgan, W. E. and Pighin, E. P. and Karlin, A. R. and Levy, H. M. and Thekkath, C. A.},
	year         = 1995,
	booktitle    = {Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles},
	location     = {Copper Mountain, Colorado, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOSP '95},
	pages        = {201–212},
	doi          = {10.1145/224056.224072},
	isbn         = {0897917154},
	url          = {https://doi.org/10.1145/224056.224072},
	numpages     = 12
}
@misc{go,
	title        = {The Go Programming Language},
	author       = {Rob Pike},
	howpublished = {\url{https://golang.org/}}
}
@misc{Grant2021InContRes,
	title        = {Anonymized workshop paper},
	author       = {Anonymous}
}
@inproceedings{helios,
	title        = {Helios: Heterogeneous Multiprocessing with Satellite Kernels},
	author       = {Nightingale, Edmund B and Hodson, Orion and McIlroy, Ross and Hawblitzel, Chris and Hunt, Galen},
	year         = 2009,
	month        = {October},
	booktitle    = {Proceedings of the 22nd Symposium on Operating Systems Principles (SOSP '09)},
	publisher    = {Association for Computing Machinery, Inc.},
	url          = {https://www.microsoft.com/en-us/research/publication/helios-heterogeneous-multiprocessing-with-satellite-kernels/},
	abstract     = {Helios is an operating system designed to simplify the task of writing, deploying, and tuning applications for heterogeneous platforms. Helios introduces satellite kernels, which export a single, uniform set of OS abstractions across CPUs of disparate architectures and performance characteristics. Access to I/O services such as ﬁle systems are made transparent via remote message passing, which extends a standard microkernel message-passing abstraction to a satellite kernel infrastructure. Helios retargets applications to available ISAs by compiling froman intermediate language. To simplify deploying and tuning application performance, Helios exposes an afﬁnity metric to developers. Afﬁnity provides a hint to the operating system about whether a process would beneﬁt from executing on the same platform as a service it depends upon.  We developed satellite kernels for an XScale programmable I/O card and for cache-coherent NUMA architectures. We ofﬂoaded several applications and operating system components, often by changing only a single line of metadata. We show up to a 28% performance improvement by ofﬂoading tasks to the XScale I/O card. On a mail-server benchmark, we show a 39% improvement in performance by automatically splitting the application among multiple NUMA domains.},
	edition      = {Proceedings of the 22nd Symposium on Operating Systems Principles (SOSP '09)}
}
@article{herd,
	title        = {Using {RDMA} Efficiently for Key-Value Services},
	author       = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
	year         = 2014,
	month        = {August},
	journal      = {SIGCOMM Comput. Commun. Rev.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 44,
	number       = 4,
	pages        = {295–306},
	doi          = {10.1145/2740070.2626299},
	issn         = {0146-4833},
	url          = {https://doi.org/10.1145/2740070.2626299},
	abstract     = {This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.},
	issue_date   = {October 2014},
	keywords     = {infiniband, ROCE, key-value stores, RDMA},
	numpages     = 12
}
@inproceedings{hopscotch,
	title        = {Hopscotch Hashing},
	author       = {Herlihy, Maurice and Shavit, Nir and Tzafrir, Moran},
	year         = 2008,
	booktitle    = {Proceedings of the 22nd International Symposium on Distributed Computing},
	location     = {Arcachon, France},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	series       = {DISC '08},
	pages        = {350–364},
	doi          = {10.1007/978-3-540-87779-0_24},
	isbn         = 9783540877783,
	url          = {https://doi.org/10.1007/978-3-540-87779-0_24},
	abstract     = {We present a new class of resizable sequential and concurrent hash map algorithms directed at both uni-processor and multicore machines. The new hopscotch algorithms are based on a novel <em>hopscotch</em>multi-phased probing and displacement technique that has the flavors of chaining, cuckoo hashing, and linear probing, all put together, yet avoids the limitations and overheads of these former approaches. The resulting algorithms provide tables with very low synchronization overheads and high cache hit ratios.In a series of benchmarks on a state-of-the-art 64-way Niagara II multicore machine, a concurrent version of hopscotch proves to be highly scalable, delivering in some cases 2 or even 3 times the throughput of today's most efficient concurrent hash algorithm, Lea's <literal>ConcurrentHashMap</literal>from <em>java.concurr.util</em>. Moreover, in tests on both Intel and Sun uni-processor machines, a sequential version of hopscotch consistently outperforms the most effective sequential hash table algorithms including cuckoo hashing and bounded linear probing.The most interesting feature of the new class of hopscotch algorithms is that they continue to deliver good performance when the hash table is more than 90% full, increasing their advantage over other algorithms as the table density grows.},
	numpages     = 15
}
@inproceedings{hpcc,
	title        = {HPCC: High Precision Congestion Control},
	author       = {Li, Yuliang and Miao, Rui and Liu, Hongqiang Harry and Zhuang, Yan and Feng, Fei and Tang, Lingbo and Cao, Zheng and Zhang, Ming and Kelly, Frank and Alizadeh, Mohammad and Yu, Minlan},
	year         = 2019,
	booktitle    = {Proceedings of the ACM Special Interest Group on Data Communication},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '19},
	pages        = {44–58},
	doi          = {10.1145/3341302.3342085},
	isbn         = 9781450359566,
	url          = {https://doi.org/10.1145/3341302.3342085},
	abstract     = {Congestion control (CC) is the key to achieving ultra-low latency, high bandwidth and network stability in high-speed networks. From years of experience operating large-scale and high-speed RDMA networks, we find the existing high-speed CC schemes have inherent limitations for reaching these goals. In this paper, we present HPCC (High Precision Congestion Control), a new high-speed CC mechanism which achieves the three goals simultaneously. HPCC leverages in-network telemetry (INT) to obtain precise link load information and controls traffic precisely. By addressing challenges such as delayed INT information during congestion and overreac-tion to INT information, HPCC can quickly converge to utilize free bandwidth while avoiding congestion, and can maintain near-zero in-network queues for ultra-low latency. HPCC is also fair and easy to deploy in hardware. We implement HPCC with commodity programmable NICs and switches. In our evaluation, compared to DCQCN and TIMELY, HPCC shortens flow completion times by up to 95%, causing little congestion even under large-scale incasts.},
	keywords     = {RDMA, congestion control, programmable switch, smart NIC},
	numpages     = 15
}
@inproceedings{hydra,
	title        = {Hydra : Resilient and Highly Available Remote Memory},
	author       = {Youngmoon Lee and Hasan Al Maruf and Mosharaf Chowdhury and Asaf Cidon and Kang G. Shin},
	year         = 2022,
	month        = {February},
	booktitle    = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
	publisher    = {USENIX Association},
	address      = {Santa Clara, CA},
	pages        = {181--198},
	isbn         = {978-1-939133-26-7},
	url          = {https://www.usenix.org/conference/fast22/presentation/lee}
}
@misc{infiniband-spec,
	title        = {Infiniband Specification},
	author       = {{Infiniband Trade Association}},
	year         = 2007,
	howpublished = {https://www.afs.enea.it/asantoro/}
}
@inproceedings{infiniswap,
	title        = {Efficient Memory Disaggregation with Infiniswap},
	author       = {Juncheng Gu and Youngmoon Lee and Yiwen Zhang and Mosharaf Chowdhury and Kang G. Shin},
	year         = 2017,
	month        = {March},
	booktitle    = {14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17)},
	publisher    = {{USENIX} Association},
	address      = {Boston, MA},
	pages        = {649--667},
	isbn         = {978-1-931971-37-9},
	url          = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/gu}
}
@misc{intel-rack,
	title        = {Intel Rack Scale Architecture: Faster Service Delivery and Lower {TCO}},
	author       = {Intel},
	howpublished = {https://www.intel.com/content/www/us/en/architecture-and-technology/rack-scale-design-overview.html}
}
@inproceedings{ironsync,
	title        = {Sharding the State Machine: Automated Modular Reasoning for Complex Concurrent Systems},
	author       = {Travis Hance and Yi Zhou and Andrea Lattuada and Reto Achermann and Alex Conway and Ryan Stutsman and Gerd Zellweger and Chris Hawblitzel and Jon Howell and Bryan Parno},
	year         = 2023,
	month        = {July},
	booktitle    = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	pages        = {911--929},
	isbn         = {978-1-939133-34-2},
	url          = {https://www.usenix.org/conference/osdi23/presentation/hance}
}
@inproceedings{ivy,
	title        = {Ivy: A Read/Write Peer-to-Peer File System},
	author       = {Athicha Muthitacharoen and Robert Morris and Thomer M. Gil and Benjie Chen},
	year         = 2002,
	month        = {December},
	booktitle    = {5th Symposium on Operating Systems Design and Implementation ({OSDI} 02)},
	publisher    = {{USENIX} Association},
	address      = {Boston, MA},
	url          = {https://www.usenix.org/conference/osdi-02/ivy-readwrite-peer-peer-file-system}
}
@misc{jemalloc,
	title        = {jemalloc, memory allocator},
	author       = {FreeBSD},
	year         = 2024,
	howpublished = {\url{http://jemalloc.net/}}
}
@inproceedings{kona,
	title        = {Rethinking Software Runtimes for Disaggregated Memory},
	author       = {Calciu, Irina and Imran, M. Talha and Puddu, Ivan and Kashyap, Sanidhya and Maruf, Hasan Al and Mutlu, Onur and Kolli, Aasheesh},
	year         = 2021,
	booktitle    = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	location     = {Virtual, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS '21},
	pages        = {79–92},
	doi          = {10.1145/3445814.3446713},
	isbn         = 9781450383172,
	url          = {https://doi.org/10.1145/3445814.3446713},
	abstract     = {Disaggregated memory can address resource provisioning inefficiencies in current datacenters. Multiple software runtimes for disaggregated memory have been proposed in an attempt to make disaggregated memory practical. These systems rely on the virtual memory subsystem to transparently offer disaggregated memory to applications using a local memory abstraction. Unfortunately, using virtual memory for disaggregation has multiple limitations, including high overhead that comes from the use of page faults to identify what data to fetch and cache locally, and high dirty data amplification that comes from the use of page-granularity for tracking changes to the cached data (4KB or higher). In this paper, we propose a fundamentally new approach to designing software runtimes for disaggregated memory that addresses these limitations. Our main observation is that we can use cache coherence instead of virtual memory for tracking applications' memory accesses transparently, at cache-line granularity. This simple idea (1) eliminates page faults from the application critical path when accessing remote data, and (2) decouples the application memory access tracking from the virtual memory page size, enabling cache-line granularity dirty data tracking and eviction. Using this observation, we implemented a new software runtime for disaggregated memory that improves average memory access time by 1.7-5X and reduces dirty data amplification by 2-10X, compared to state-of-the-art systems.},
	keywords     = {remote memory, disaggregated memory, cache coherence},
	numpages     = 14
}
@inproceedings{kv-direct,
	title        = {KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC},
	author       = {Li, Bojie and Ruan, Zhenyuan and Xiao, Wencong and Lu, Yuanwei and Xiong, Yongqiang and Putnam, Andrew and Chen, Enhong and Zhang, Lintao},
	year         = 2017,
	month        = {October},
	booktitle    = {Proceedings of the 26th Symposium on Operating Systems Principles},
	publisher    = {ACM},
	pages        = {137--152},
	isbn         = {978-1-4503-5085-3},
	url          = {https://www.microsoft.com/en-us/research/publication/kv-direct-high-performance-memory-key-value-store-programmable-nic/},
	abstract     = {
		Performance of in-memory key-value store (KVS) continues to be of great importance as modern KVS goes beyond the traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation in data centers. Recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most KVS from the network to the CPU. RDMA-capable NIC partly alleviates the problem, but the primitives provided by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling in-network processing. In this paper, we present KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory.

		We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 μs. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.
	},
	edition      = {Proceedings of the 26th Symposium on Operating Systems Principles}
}
@inproceedings{leap,
	title        = {Effectively Prefetching Remote Memory with {Leap}},
	author       = {Hasan Al Maruf and Mosharaf Chowdhury},
	year         = 2020,
	month        = {July},
	booktitle    = {2020 {USENIX} Annual Technical Conference ({USENIX} {ATC} 20)},
	publisher    = {{USENIX} Association},
	pages        = {843--857},
	isbn         = {978-1-939133-14-4},
	url          = {https://www.usenix.org/conference/atc20/presentation/al-maruf}
}
@inproceedings{legoos,
	title        = {{LegoOS}: A Disseminated, Distributed {OS} for Hardware Resource Disaggregation},
	author       = {Yizhou Shan and Yutong Huang and Yilun Chen and Yiying Zhang},
	year         = 2018,
	booktitle    = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
	publisher    = {{USENIX} Association},
	address      = {Carlsbad, CA},
	pages        = {69--87},
	isbn         = {978-1-931971-47-8},
	url          = {https://www.usenix.org/conference/osdi18/presentation/shan}
}
@inproceedings{lite,
	title        = {{LITE} Kernel {RDMA} Support for Datacenter Applications},
	author       = {Tsai, Shin-Yeh and Zhang, Yiying},
	year         = 2017,
	booktitle    = {Proceedings of the 26th Symposium on Operating Systems Principles},
	publisher    = {Association for Computing Machinery},
	address      = {Shanghai, China},
	pages        = {306–324},
	doi          = {10.1145/3132747.3132762},
	isbn         = 9781450350853,
	url          = {https://doi.org/10.1145/3132747.3132762},
	keywords     = {RDMA, indirection, low-latency network, network stack},
	numpages     = 19
}
@article{Mansour_2019,
	title        = {{FPGA} Implementation of {RDMA-Based} Data Acquisition System Over {100-Gb Ethernet}},
	author       = {Mansour, Wassim and Janvier, Nicolas and Fajardo, Pablo},
	year         = 2019,
	month        = {Jul},
	journal      = {IEEE Transactions on Nuclear Science},
	publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
	volume       = 66,
	number       = 7,
	pages        = {1138–1143},
	doi          = {10.1109/tns.2019.2904118},
	issn         = {1558-1578},
	url          = {http://dx.doi.org/10.1109/TNS.2019.2904118}
}
@inproceedings{memc3,
	title        = {{MemC3}: Compact and Concurrent MemCache with Dumber Caching and Smarter Hashing},
	author       = {Bin Fan and David G. Andersen and Michael Kaminsky},
	year         = 2013,
	month        = {April},
	booktitle    = {10th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 13)},
	publisher    = {{USENIX} Association},
	address      = {Lombard, IL},
	pages        = {371--384},
	isbn         = {978-1-931971-00-3},
	url          = {https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/fan}
}
@misc{memcached,
	title        = {{memcached: a Distributed Memory Object Caching System}},
	author       = {Memcached},
	howpublished = {\url{http://www.memcached.org/}}
}
@inproceedings{mesos,
	title        = {Mesos: A Platform for {Fine-Grained} Resource Sharing in the Data Center},
	author       = {Benjamin Hindman and Andy Konwinski and Matei Zaharia and Ali Ghodsi and Anthony D. Joseph and Randy Katz and Scott Shenker and Ion Stoica},
	year         = 2011,
	month        = {March},
	booktitle    = {8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	url          = {https://www.usenix.org/conference/nsdi11/mesos-platform-fine-grained-resource-sharing-data-center}
}
@inproceedings{mica,
	title        = {{MICA}: A Holistic Approach to Fast {In-Memory} {Key-Value} Storage},
	author       = {Hyeontaek Lim and Dongsu Han and David G. Andersen and Michael Kaminsky},
	year         = 2014,
	month        = {April},
	booktitle    = {11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14)},
	publisher    = {USENIX Association},
	address      = {Seattle, WA},
	pages        = {429--444},
	isbn         = {978-1-931971-09-6},
	url          = {https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/lim}
}
@misc{microsoft-cxl-first-gen,
	title        = {First-generation Memory Disaggregation for Cloud Platforms},
	author       = {Li, Huaicheng and Berger, Daniel S. and Novakovic, Stanko and Hsu, Lisa and Ernst, Dan and Zardoshti, Pantea and Shah, Monish and Agarwal, Ishwar and Hill, Mark D. and Fontoura, Marcus and Bianchini, Ricardo},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2203.00241},
	url          = {https://arxiv.org/abs/2203.00241},
	copyright    = {Creative Commons Attribution Share Alike 4.0 International},
	keywords     = {Operating Systems (cs.OS), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{mind,
	title        = {MIND: In-Network Memory Management for Disaggregated Data Centers},
	author       = {Lee, Seung-seob and Yu, Yanpeng and Tang, Yupeng and Khandelwal, Anurag and Zhong, Lin and Bhattacharjee, Abhishek},
	year         = 2021,
	booktitle    = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
	location     = {Virtual Event, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOSP '21},
	pages        = {488–504},
	doi          = {10.1145/3477132.3483561},
	isbn         = 9781450387095,
	url          = {https://doi.org/10.1145/3477132.3483561},
	abstract     = {Memory disaggregation promises transparent elasticity, high resource utilization and hardware heterogeneity in data centers by physically separating memory and compute into network-attached resource "blades". However, existing designs achieve performance at the cost of resource elasticity, restricting memory sharing to a single compute blade to avoid costly memory coherence traffic over the network.In this work, we show that emerging programmable network switches can enable an efficient shared memory abstraction for disaggregated architectures by placing memory management logic in the network fabric. We find that centralizing memory management in the network permits bandwidth and latency-efficient realization of in-network cache coherence protocols, while programmable switch ASICs support other memory management logic at line-rate. We realize these insights into MIND1, an in-network memory management unit for rack-scale disaggregation. MIND enables transparent resource elasticity while matching the performance of prior memory disaggregation proposals for real-world workloads.},
	keywords     = {Memory disaggregation, Programmable networks},
	numpages     = 17
}
@inproceedings{mira,
	title        = {Mira: A Program-Behavior-Guided Far Memory System},
	author       = {Guo, Zhiyuan and He, Zijian and Zhang, Yiying},
	year         = 2023,
	booktitle    = {Proceedings of the 29th Symposium on Operating Systems Principles},
	location     = {, Koblenz, Germany,},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOSP '23},
	pages        = {692–708},
	doi          = {10.1145/3600006.3613157},
	isbn         = 9798400702297,
	url          = {https://doi.org/10.1145/3600006.3613157},
	abstract     = {Far memory, where memory accesses are non-local, has become more popular in recent years as a solution to expand memory size and avoid memory stranding. Prior far memory systems have taken two approaches: transparently swap memory pages between local and far memory, and utilizing new programming models to explicitly move fine-grained data between local and far memory. The former requires no program changes but comes with performance penalty. The latter has potentially better performance but requires significant program changes.We propose a new far-memory approach by automatically inferring program behavior and efficiently utilizing it to improve application performance. With this idea, we build Mira. Mira utilizes program analysis results, profiled execution information, and system environments together to guide code compilation and system configurations for far memory. Our evaluation shows that Mira outperforms prior swap-based and programming-model-based systems by up to 18 times.},
	numpages     = 17
}
@inproceedings{mom,
	title        = {Designing Distributed Systems Using Approximate Synchrony in Data Center Networks},
	author       = {Ports, Dan R. K. and Li, Jialin and Liu, Vincent and Sharma, Naveen Kr. and Krishnamurthy, Arvind},
	year         = 2015,
	booktitle    = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
	location     = {Oakland, CA},
	publisher    = {USENIX Association},
	address      = {USA},
	series       = {NSDI'15},
	pages        = {43–57},
	isbn         = 9781931971218,
	abstract     = {Distributed systems are traditionally designed independently from the underlying network, making worst-case assumptions (e.g., complete asynchrony) about its behavior. However, many of today's distributed applications are deployed in data centers, where the network is more reliable, predictable, and extensible. In these environments, it is possible to co-design distributed systems with their network layer, and doing so can offer substantial benefits.This paper explores network-level mechanisms for providing Mostly-Ordered Multicast (MOM): a best-effort ordering property for concurrent multicast operations. Using this primitive, we design Speculative Paxos, a state machine replication protocol that relies on the network to order requests in the normal case. This approach leads to substantial performance benefits: under realistic data center conditions, Speculative Paxos can provide 40% lower latency and 2.6\texttimes{} higher throughput than the standard Paxos protocol. It offers lower latency than a latency-optimized protocol (Fast Paxos) with the same throughput as a throughput-optimized protocol (batching).},
	numpages     = 15
}
@inproceedings{netcache,
	title        = {NetCache: Balancing Key-Value Stores with Fast In-Network Caching},
	author       = {Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Soul\'{e}, Robert and Lee, Jeongkeun and Foster, Nate and Kim, Changhoon and Stoica, Ion},
	year         = 2017,
	booktitle    = {Proceedings of the 26th Symposium on Operating Systems Principles},
	location     = {Shanghai, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOSP '17},
	pages        = {121–136},
	doi          = {10.1145/3132747.3132764},
	isbn         = 9781450350853,
	url          = {https://doi.org/10.1145/3132747.3132764},
	abstract     = {We present NetCache, a new key-value store architecture that leverages the power and flexibility of new-generation programmable switches to handle queries on hot items and balance the load across storage nodes. NetCache provides high aggregate throughput and low latency even under highly-skewed and rapidly-changing workloads. The core of NetCache is a packet-processing pipeline that exploits the capabilities of modern programmable switch ASICs to efficiently detect, index, cache and serve hot key-value items in the switch data plane. Additionally, our solution guarantees cache coherence with minimal overhead. We implement a NetCache prototype on Barefoot Tofino switches and commodity servers and demonstrate that a single switch can process 2+ billion queries per second for 64K items with 16-byte keys and 128-byte values, while only consuming a small portion of its hardware resources. To the best of our knowledge, this is the first time that a sophisticated application-level functionality, such as in-network caching, has been shown to run at line rate on programmable switches. Furthermore, we show that NetCache improves the throughput by 3-10x and reduces the latency of up to 40% of queries by 50%, for high-performance, in-memory key-value stores.},
	keywords     = {Programmable switches, Caching, Key-value stores},
	numpages     = 16
}
@inproceedings{netchain,
	title        = {{NetChain}: {Scale-Free} {Sub-RTT} Coordination},
	author       = {Xin Jin and Xiaozhou Li and Haoyu Zhang and Nate Foster and Jeongkeun Lee and Robert Soul{\'e} and Changhoon Kim and Ion Stoica},
	year         = 2018,
	month        = {April},
	booktitle    = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)},
	publisher    = {USENIX Association},
	address      = {Renton, WA},
	pages        = {35--49},
	isbn         = {978-1-939133-01-4},
	url          = {https://www.usenix.org/conference/nsdi18/presentation/jin}
}
@inproceedings{netkv,
	title        = {{NetKV}: Scalable, Self-Managing, Load Balancing as a Network Function},
	author       = {Zhang, Wei and Wood, Timothy and Hwang, Jinho},
	year         = 2016,
	booktitle    = {2016 IEEE International Conference on Autonomic Computing (ICAC)},
	volume       = {},
	number       = {},
	pages        = {5--14},
	doi          = {10.1109/ICAC.2016.28}
}
@inproceedings{netlock,
	title        = {NetLock: Fast, Centralized Lock Management Using Programmable Switches},
	author       = {Yu, Zhuolong and Zhang, Yiwen and Braverman, Vladimir and Chowdhury, Mosharaf and Jin, Xin},
	year         = 2020,
	booktitle    = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '20},
	pages        = {126–138},
	doi          = {10.1145/3387514.3405857},
	isbn         = 9781450379557,
	url          = {https://doi.org/10.1145/3387514.3405857},
	abstract     = {Lock managers are widely used by distributed systems. Traditional centralized lock managers can easily support policies between multiple users using global knowledge, but they suffer from low performance. In contrast, emerging decentralized approaches are faster but cannot provide flexible policy support. Furthermore, performance in both cases is limited by the server capability.We present NetLock, a new centralized lock manager that co-designs servers and network switches to achieve high performance without sacrificing flexibility in policy support. The key idea of NetLock is to exploit the capability of emerging programmable switches to directly process lock requests in the switch data plane. Due to the limited switch memory, we design a memory management mechanism to seamlessly integrate the switch and server memory. To realize the locking functionality in the switch, we design a custom data plane module that efficiently pools multiple register arrays together to maximize memory utilization We have implemented a NetLock prototype with a Barefoot Tofino switch and a cluster of commodity servers. Evaluation results show that NetLock improves the throughput by 14.0-18.4x, and reduces the average and 99% latency by 4.7-20.3x and 10.4-18.7x over DSLR, a state-of-the-art RDMA-based solution, while providing flexible policy support.},
	keywords     = {Data plane, Lock Management, Programmable Switches, Centralized},
	numpages     = 13
}
@inproceedings{no,
	title        = {Just Say {NO} to Paxos Overhead: Replacing Consensus with Network Ordering},
	author       = {Jialin Li and Ellis Michael and Naveen Kr. Sharma and Adriana Szekeres and Dan R. K. Ports},
	year         = 2016,
	month        = {November},
	booktitle    = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
	publisher    = {USENIX Association},
	address      = {Savannah, GA},
	pages        = {467--483},
	isbn         = {978-1-931971-33-1},
	url          = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/li}
}
@inproceedings{nonb-binary,
	title        = {Non-Blocking Binary Search Trees},
	author       = {Ellen, Faith and Fatourou, Panagiota and Ruppert, Eric and van Breugel, Franck},
	year         = 2010,
	booktitle    = {Proceedings of the 29th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
	location     = {Zurich, Switzerland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {PODC '10},
	pages        = {131–140},
	doi          = {10.1145/1835698.1835736},
	isbn         = 9781605588889,
	url          = {https://doi.org/10.1145/1835698.1835736},
	abstract     = {This paper describes the first complete implementation of a non-blocking binary search tree in an asynchronous shared-memory system using single-word compare-and-swap operations. The implementation is linearizable and tolerates any number of crash failures. Insert and Delete operations that modify different parts of the tree do not interfere with one another, so they can run completely concurrently. Find operations only perform reads of shared memory.},
	keywords     = {binary search tree, cas, non-blocking, shared memory},
	numpages     = 10
}
@inproceedings{nros,
	title        = {NrOS: Effective Replication and Sharing in an Operating System},
	author       = {Ankit Bhardwaj and Chinmay Kulkarni and Reto Achermann and Irina Calciu and Sanidhya Kashyap and Ryan Stutsman and Amy Tai and Gerd Zellweger},
	year         = 2021,
	month        = {July},
	booktitle    = {15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
	publisher    = {{USENIX} Association},
	pages        = {295--312},
	isbn         = {978-1-939133-22-9},
	url          = {https://www.usenix.org/conference/osdi21/presentation/bhardwaj}
}
@inproceedings{omega,
	title        = {Omega: flexible, scalable schedulers for large compute clusters},
	author       = {Malte Schwarzkopf and Andy Konwinski and Michael Abd-El-Malek and John Wilkes},
	year         = 2013,
	booktitle    = {SIGOPS European Conference on Computer Systems (EuroSys)},
	address      = {Prague, Czech Republic},
	pages        = {351--364},
	url          = {http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf}
}
@misc{omni-path,
	title        = {Intel Omni-Path Architecture},
	author       = {Intel},
	howpublished = {https://tinyurl.com/ya3x4ktd}
}
@inproceedings{one-rma,
	title        = {1RMA: Re-Envisioning Remote Memory Access for Multi-Tenant Datacenters},
	author       = {Singhvi, Arjun and Akella, Aditya and Gibson, Dan and Wenisch, Thomas F. and Wong-Chan, Monica and Clark, Sean and Martin, Milo M. K. and McLaren, Moray and Chandra, Prashant and Cauble, Rob and Wassel, Hassan M. G. and Montazeri, Behnam and Sabato, Simon L. and Scherpelz, Joel and Vahdat, Amin},
	year         = 2020,
	booktitle    = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '20},
	pages        = {708–721},
	doi          = {10.1145/3387514.3405897},
	isbn         = 9781450379557,
	url          = {https://doi.org/10.1145/3387514.3405897},
	abstract     = {Remote Direct Memory Access (RDMA) plays a key role in supporting performance-hungry datacenter applications. However, existing RDMA technologies are ill-suited to multi-tenant datacenters, where applications run at massive scales, tenants require isolation and security, and the workload mix changes over time. Our experiences seeking to operationalize RDMA at scale indicate that these ills are rooted in standard RDMA's basic design attributes: connectionorientedness and complex policies baked into hardware.We describe a new approach to remote memory access -- One-Shot RMA (1RMA) -- suited to the constraints imposed by our multi-tenant datacenter settings. The 1RMA NIC is connection-free and fixed-function; it treats each RMA operation independently, assisting software by offering fine-grained delay measurements and fast failure notifications. 1RMA software provides operation pacing, congestion control, failure recovery, and inter-operation ordering, when needed. The NIC, deployed in our production datacenters, supports encryption at line rate (100Gbps and 100M ops/sec) with minimal performance/availability disruption for encryption key rotation.},
	keywords     = {Remote Memory Access, Connection Free, Congestion Control},
	numpages     = 14
}
@inproceedings{p4-telem,
	title        = {Using P4 and RDMA to collect telemetry data},
	author       = {Beltman, Rutger and Knossen, Silke and Hill, Joseph and Grosso, Paola},
	year         = 2020,
	booktitle    = {2020 IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS)},
	volume       = {},
	number       = {},
	pages        = {1--9},
	doi          = {10.1109/INDIS51933.2020.00006}
}
@article{p4telemetry,
	title        = {Using P4 and RDMA to collect telemetry data},
	author       = {Rutger Beltman and Silke Knossen and Joseph Hill and Paola Grosso},
	year         = 2020,
	journal      = {2020 IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS)},
	pages        = {1--9}
}
@inproceedings{pilaf,
	title        = {Using One-Sided {RDMA} Reads to Build a Fast, {CPU}-Efficient Key-Value Store},
	author       = {Christopher Mitchell and Yifeng Geng and Jinyang Li},
	year         = 2013,
	month        = {June},
	booktitle    = {2013 {USENIX} Annual Technical Conference ({USENIX} {ATC} 13)},
	publisher    = {{USENIX} Association},
	address      = {San Jose, CA},
	pages        = {103--114},
	isbn         = {978-1-931971-01-0},
	url          = {https://www.usenix.org/conference/atc13/technical-sessions/presentation/mitchell}
}
@article{pointer-chaising,
	title        = {An empirical evaluation of in-memory multi-version concurrency control},
	author       = {Wu, Yingjun and Arulraj, Joy and Lin, Jiexi and Xian, Ran and Pavlo, Andrew},
	year         = 2017,
	month        = {03},
	journal      = {Proceedings of the VLDB Endowment},
	volume       = 10,
	pages        = {781--792},
	doi          = {10.14778/3067421.3067427}
}
@inproceedings{prism,
	title        = {PRISM: Rethinking the RDMA Interface for Distributed Systems},
	author       = {Dharanipragada, Sowmya and Joyner, Shannon and Burke, Matthew and Szekeres, Adriana and Nelson, Jacob and Zhang, Irene and Ports, Dan R. K.},
	year         = 2021,
	month        = {October},
	booktitle    = {SOSP 2021},
	url          = {https://www.microsoft.com/en-us/research/publication/prism-rethinking-the-rdma-interface-for-distributed-systems/},
	abstract     = {Remote Direct Memory Access (RDMA) has been used to accelerate a variety of distributed systems, by providing low-latency, CPU-bypassing access to a remote host's memory. However, most of the distributed protocols used in these systems cannot easily be expressed in terms of the simple memory READs and WRITEs provided by RDMA. As a result, designers face a choice between introducing additional protocol complexity (e.g., additional round trips) or forgoing the benefits of RDMA entirely. This paper argues that an extension to the RDMA interface can resolve this dilemma. We introduce the PRISM interface, which extends the RDMA interface with four new primitives: indirection, allocation, enhanced compare-and-swap, and operation chaining. These increase the expressivity of the RDMA interface, while still being implementable using the same underlying hardware features. We show their utility by designing three new applications using PRISM primitives, that require little to no server-side CPU involvement: (1) PRISM-KV, a key-value store; (2) PRISM-RS a replicated block store; and (3) PRISM-TX, a distributed transaction protocol. Using a software-based implementation of the PRISM primitives, we show that these systems outperform prior RDMA-based equivalents.}
}
@inproceedings{r2p2,
	title        = {{R2P2}: Making {RPCs} first-class datacenter citizens},
	author       = {Marios Kogias and George Prekas and Adrien Ghosn and Jonas Fietz and Edouard Bugnion},
	year         = 2019,
	month        = {July},
	booktitle    = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
	publisher    = {USENIX Association},
	address      = {Renton, WA},
	pages        = {863--880},
	isbn         = {978-1-939133-03-8},
	url          = {https://www.usenix.org/conference/atc19/presentation/kogias-r2p2}
}
@inproceedings{race,
	title        = {One-sided {RDMA-Conscious} Extendible Hashing for Disaggregated Memory},
	author       = {Pengfei Zuo and Jiazhao Sun and Liu Yang and Shuangwu Zhang and Yu Hua},
	year         = 2021,
	month        = {July},
	booktitle    = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
	publisher    = {USENIX Association},
	pages        = {15--29},
	isbn         = {978-1-939133-23-6},
	url          = {https://www.usenix.org/conference/atc21/presentation/zuo}
}
@inproceedings{racksched,
	title        = {{RackSched}: A {Microsecond-Scale} Scheduler for {Rack-Scale} Computers},
	author       = {Hang Zhu and Kostis Kaffes and Zixu Chen and Zhenming Liu and Christos Kozyrakis and Ion Stoica and Xin Jin},
	year         = 2020,
	month        = {November},
	booktitle    = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
	publisher    = {USENIX Association},
	pages        = {1225--1240},
	isbn         = {978-1-939133-19-9},
	url          = {https://www.usenix.org/conference/osdi20/presentation/zhu}
}
@article{ramcloud,
	title        = {The RAMCloud Storage System},
	author       = {Ousterhout, John and Gopalan, Arjun and Gupta, Ashish and Kejriwal, Ankita and Lee, Collin and Montazeri, Behnam and Ongaro, Diego and Park, Seo Jin and Qin, Henry and Rosenblum, Mendel and Rumble, Stephen and Stutsman, Ryan and Yang, Stephen},
	year         = 2015,
	month        = {aug},
	journal      = {ACM Trans. Comput. Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 33,
	number       = 3,
	doi          = {10.1145/2806887},
	issn         = {0734-2071},
	url          = {https://doi.org/10.1145/2806887},
	abstract     = {RAMCloud is a storage system that provides low-latency access to large-scale datasets. To achieve low latency, RAMCloud stores all data in DRAM at all times. To support large capacities (1PB or more), it aggregates the memories of thousands of servers into a single coherent key-value store. RAMCloud ensures the durability of DRAM-based data by keeping backup copies on secondary storage. It uses a uniform log-structured mechanism to manage both DRAM and secondary storage, which results in high performance and efficient memory usage. RAMCloud uses a polling-based approach to communication, bypassing the kernel to communicate directly with NICs; with this approach, client applications can read small objects from any RAMCloud storage server in less than 5μs, durable writes of small objects take about 13.5μs. RAMCloud does not keep multiple copies of data online; instead, it provides high availability by recovering from crashes very quickly (1 to 2 seconds). RAMCloud’s crash recovery mechanism harnesses the resources of the entire cluster working concurrently so that recovery performance scales with cluster size.},
	articleno    = 7,
	issue_date   = {September 2015},
	keywords     = {storage systems, low latency, large-scale systems, Datacenters},
	numpages     = 55
}
@misc{rdma-latency,
	title        = {RoCE vs. iWARP Competitive Analysis},
	author       = {NVIDIA},
	howpublished = {\url{https://network.nvidia.com/pdf/whitepapers/WP_RoCE_vs_iWARP.pdf}}
}
@misc{rdma-masked-cas,
	title        = {Advanced Transport},
	author       = {NVIDIA},
	howpublished = {\url{https://docs.nvidia.com/networking/display/MLNXOFEDv494170/Advanced+Transport}}
}
@inproceedings{redmark,
	title        = {{ReDMArk}: Bypassing {RDMA} Security Mechanisms},
	author       = {Benjamin Rothenberger and Konstantin Taranov and Adrian Perrig and Torsten Hoefler},
	year         = 2021,
	month        = {August},
	booktitle    = {30th USENIX Security Symposium (USENIX Security 21)},
	publisher    = {USENIX Association},
	pages        = {4277--4292},
	isbn         = {978-1-939133-24-3},
	url          = {https://www.usenix.org/conference/usenixsecurity21/presentation/rothenberger}
}
@inproceedings{regions,
	title        = {Remote regions: a simple abstraction for remote memory},
	author       = {Marcos K. Aguilera and Nadav Amit and Irina Calciu and Xavier Deguillard and Jayneel Gandhi and Stanko Novakovi{\'c} and Arun Ramanathan and Pratap Subrahmanyam and Lalith Suresh and Kiran Tati and Rajesh Venkatasubramanian and Michael Wei},
	year         = 2018,
	month        = {July},
	booktitle    = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	pages        = {775--787},
	isbn         = {978-1-939133-01-4},
	url          = {https://www.usenix.org/conference/atc18/presentation/aguilera}
}
@inproceedings{reigons,
	title        = {Remote regions: a simple abstraction for remote memory},
	author       = {Marcos K. Aguilera and Nadav Amit and Irina Calciu and Xavier Deguillard and Jayneel Gandhi and Stanko Novakovi{\'c} and Arun Ramanathan and Pratap Subrahmanyam and Lalith Suresh and Kiran Tati and Rajesh Venkatasubramanian and Michael Wei},
	year         = 2018,
	month        = {July},
	booktitle    = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)},
	publisher    = {{USENIX} Association},
	address      = {Boston, MA},
	pages        = {775--787},
	isbn         = {978-1-939133-01-4},
	url          = {https://www.usenix.org/conference/atc18/presentation/aguilera}
}
@inproceedings{reno,
	title        = {Reno: An RDMA-Enabled, Non-Volatile Memory-Optimized Key-Value Store},
	author       = {Huang, Rulin and Huang, Kaixin and Wang, Jingyu and Chen, Yuting},
	year         = 2021,
	booktitle    = {2021 IEEE 27th International Conference on Parallel and Distributed Systems (ICPADS)},
	volume       = {},
	number       = {},
	pages        = {466--473},
	doi          = {10.1109/ICPADS53394.2021.00064}
}
@inproceedings{requirements,
	title        = {Network Requirements for Resource Disaggregation},
	author       = {Peter X. Gao and Akshay Narayan and Sagar Karandikar and Joao Carreira and Sangjin Han and Rachit Agarwal and Sylvia Ratnasamy and Scott Shenker},
	year         = 2016,
	month        = {November},
	booktitle    = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
	publisher    = {{USENIX} Association},
	address      = {Savannah, GA},
	pages        = {249--264},
	isbn         = {978-1-931971-33-1},
	url          = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gao}
}
@inproceedings{rethinking,
	title        = {Rethinking Software Runtimes for Disaggregated Memory},
	author       = {Calciu, Irina and Imran, M. Talha and Puddu, Ivan and Kashyap, Sanidhya and Maruf, Hasan Al and Mutlu, Onur and Kolli, Aasheesh},
	year         = 2021,
	booktitle    = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	location     = {Virtual, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS 2021},
	pages        = {79–92},
	doi          = {10.1145/3445814.3446713},
	isbn         = 9781450383172,
	url          = {https://doi.org/10.1145/3445814.3446713},
	abstract     = {Disaggregated memory can address resource provisioning inefficiencies in current datacenters. Multiple software runtimes for disaggregated memory have been proposed in an attempt to make disaggregated memory practical. These systems rely on the virtual memory subsystem to transparently offer disaggregated memory to applications using a local memory abstraction. Unfortunately, using virtual memory for disaggregation has multiple limitations, including high overhead that comes from the use of page faults to identify what data to fetch and cache locally, and high dirty data amplification that comes from the use of page-granularity for tracking changes to the cached data (4KB or higher).  In this paper, we propose a fundamentally new approach to designing software runtimes for disaggregated memory that addresses these limitations. Our main observation is that we can use cache coherence instead of virtual memory for tracking applications' memory accesses transparently, at cache-line granularity. This simple idea (1) eliminates page faults from the application critical path when accessing remote data, and (2) decouples the application memory access tracking from the virtual memory page size, enabling cache-line granularity dirty data tracking and eviction. Using this observation, we implemented a new software runtime for disaggregated memory that improves average memory access time by 1.7-5X and reduces dirty data amplification by 2-10X, compared to state-of-the-art systems.},
	keywords     = {remote memory, disaggregated memory, cache coherence},
	numpages     = 14
}
@misc{rfc3135,
	title        = {{Performance Enhancing Proxies Intended to Mitigate Link-Related Degradations}},
	author       = {Jim Griner and John Border and Markku Kojo and Zach D. Shelby and Gabriel Montenegro},
	year         = 2001,
	month        = {June},
	publisher    = {RFC Editor},
	series       = {Request for Comments},
	number       = 3135,
	doi          = {10.17487/RFC3135},
	url          = {https://www.rfc-editor.org/info/rfc3135},
	abstract     = {This document is a survey of Performance Enhancing Proxies (PEPs) often employed to improve degraded TCP performance caused by characteristics of specific link environments, for example, in satellite, wireless WAN, and wireless LAN environments. This memo provides information for the Internet community.},
	howpublished = {RFC 3135},
	pagetotal    = 45
}
@inproceedings{rma,
	title        = {1RMA: Re-Envisioning Remote Memory Access for Multi-Tenant Datacenters},
	author       = {Singhvi, Arjun and Akella, Aditya and Gibson, Dan and Wenisch, Thomas F. and Wong-Chan, Monica and Clark, Sean and Martin, Milo M. K. and McLaren, Moray and Chandra, Prashant and Cauble, Rob and Wassel, Hassan M. G. and Montazeri, Behnam and Sabato, Simon L. and Scherpelz, Joel and Vahdat, Amin},
	year         = 2020,
	booktitle    = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '20},
	pages        = {708–721},
	doi          = {10.1145/3387514.3405897},
	isbn         = 9781450379557,
	url          = {https://doi.org/10.1145/3387514.3405897},
	abstract     = {Remote Direct Memory Access (RDMA) plays a key role in supporting performance-hungry datacenter applications. However, existing RDMA technologies are ill-suited to multi-tenant datacenters, where applications run at massive scales, tenants require isolation and security, and the workload mix changes over time. Our experiences seeking to operationalize RDMA at scale indicate that these ills are rooted in standard RDMA's basic design attributes: connectionorientedness and complex policies baked into hardware.We describe a new approach to remote memory access -- One-Shot RMA (1RMA) -- suited to the constraints imposed by our multi-tenant datacenter settings. The 1RMA NIC is connection-free and fixed-function; it treats each RMA operation independently, assisting software by offering fine-grained delay measurements and fast failure notifications. 1RMA software provides operation pacing, congestion control, failure recovery, and inter-operation ordering, when needed. The NIC, deployed in our production datacenters, supports encryption at line rate (100Gbps and 100M ops/sec) with minimal performance/availability disruption for encryption key rotation.},
	keywords     = {Connection Free, Remote Memory Access, Congestion Control},
	numpages     = 14
}
@inproceedings{rocks-db-workload,
	title        = {Characterizing, Modeling, and Benchmarking {RocksDB} {Key-Value} Workloads at Facebook},
	author       = {Zhichao Cao and Siying Dong and Sagar Vemuri and David H.C. Du},
	year         = 2020,
	month        = {February},
	booktitle    = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
	publisher    = {USENIX Association},
	address      = {Santa Clara, CA},
	pages        = {209--223},
	isbn         = {978-1-939133-12-0},
	url          = {https://www.usenix.org/conference/fast20/presentation/cao-zhichao}
}
@inproceedings{rolex,
	title        = {{ROLEX}: A Scalable {RDMA-oriented} Learned {Key-Value} Store for Disaggregated Memory Systems},
	author       = {Pengfei Li and Yu Hua and Pengfei Zuo and Zhangyu Chen and Jiajie Sheng},
	year         = 2023,
	month        = {February},
	booktitle    = {21st USENIX Conference on File and Storage Technologies (FAST 23)},
	publisher    = {USENIX Association},
	address      = {Santa Clara, CA},
	pages        = {99--114},
	isbn         = {978-1-939133-32-8},
	url          = {https://www.usenix.org/conference/fast23/presentation/li-pengfei}
}
@inproceedings{scalable-locks,
	title        = {A Scalable Distributed Lock Manager using One-Sided RDMA Atomic Operations},
	author       = {Sarah Zehnder},
	year         = 2015
}
@inproceedings{scalerpc,
	title        = {Scalable RDMA RPC on Reliable Connection with Efficient Resource Sharing},
	author       = {Chen, Youmin and Lu, Youyou and Shu, Jiwu},
	year         = 2019,
	booktitle    = {Proceedings of the Fourteenth EuroSys Conference 2019},
	location     = {Dresden, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {EuroSys '19},
	doi          = {10.1145/3302424.3303968},
	isbn         = 9781450362818,
	url          = {https://doi.org/10.1145/3302424.3303968},
	articleno    = 19,
	keywords     = {RDMA, Scalability, Resource Sharing},
	numpages     = 14
}
@inproceedings{Schroeder:2007:DFR:1267903.1267904,
	title        = {Disk Failures in the Real World: What Does an MTTF of 1,000,000 Hours Mean to You?},
	author       = {Schroeder, Bianca and Gibson, Garth A.},
	year         = 2007,
	booktitle    = {Proceedings of the 5th USENIX Conference on File and Storage Technologies},
	location     = {San Jose, CA},
	publisher    = {USENIX Association},
	address      = {Berkeley, CA, USA},
	series       = {FAST '07},
	url          = {http://dl.acm.org/citation.cfm?id=1267903.1267904},
	acmid        = 1267904,
	articleno    = 1
}
@inproceedings{shenango,
	title        = {Shenango: Achieving High {CPU} Efficiency for Latency-sensitive Datacenter Workloads},
	author       = {Amy Ousterhout and Joshua Fried and Jonathan Behrens and Adam Belay and Hari Balakrishnan},
	year         = 2019,
	month        = {February},
	booktitle    = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	pages        = {361--378},
	isbn         = {978-1-931971-49-2},
	url          = {https://www.usenix.org/conference/nsdi19/presentation/ousterhout}
}
@inproceedings{sherman,
	title        = {Sherman: A Write-Optimized Distributed {B+Tree} Index on Disaggregated Memory},
	author       = {Wang, Qing and Lu, Youyou and Shu, Jiwu},
	year         = 2022,
	booktitle    = {Proceedings of the 2022 International Conference on Management of Data},
	publisher    = {Association for Computing Machinery},
	address      = {Philadelphia, PA},
	pages        = {1033–1048},
	doi          = {10.1145/3514221.3517824},
	url          = {https://doi.org/10.1145/3514221.3517824},
	keywords     = {index, disaggregated memory, RDMA},
	numpages     = 16
}
@inproceedings{shinjuku,
	title        = {Shinjuku: Preemptive Scheduling for {usecond-scale} Tail Latency},
	author       = {Kostis Kaffes and Timothy Chong and Jack Tigar Humphries and Adam Belay and David Mazi{\`e}res and Christos Kozyrakis},
	year         = 2019,
	month        = {February},
	booktitle    = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
	publisher    = {USENIX Association},
	address      = {Boston, MA},
	pages        = {345--360},
	isbn         = {978-1-931971-49-2},
	url          = {https://www.usenix.org/conference/nsdi19/presentation/kaffes}
}
@inproceedings{snap,
	title        = {Snap: a Microkernel Approach to Host Networking},
	author       = {Michael Marty and Marc de Kruijf and Jacob Adriaens and Christopher Alfeld and Sean Bauer and Carlo Contavalli and Mike Dalton and Nandita Dukkipati and William C. Evans and Steve Gribble and Nicholas Kidd and Roman Kononov and Gautam Kumar and Carl Mauer and Emily Musick and Lena Olson and Mike Ryan and Erik Rubow and Kevin Springborn and Paul Turner and Valas Valancius and Xi Wang and Amin Vahdat},
	year         = 2019,
	booktitle    = {In ACM SIGOPS 27th Symposium on Operating Systems Principles},
	address      = {New York, NY, USA}
}
@inproceedings{snoop,
	title        = {Improving TCP/IP Performance over Wireless Networks},
	author       = {Balakrishnan, Hari and Seshan, Srinivasan and Amir, Elan and Katz, Randy H.},
	year         = 1995,
	booktitle    = {Proceedings of the 1st Annual International Conference on Mobile Computing and Networking},
	location     = {Berkeley, California, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MobiCom '95},
	pages        = {2–11},
	doi          = {10.1145/215530.215544},
	isbn         = {0897918142},
	url          = {https://doi.org/10.1145/215530.215544},
	abstract     = {TCP is a reliable transport protocol tuned to perform well in traditional networks made up of links with low bit-error rates. Networks with higher bit-error rates, such as those with wireless links and mobile hosts, violate many of the assumptions made by TCP, causing degraded end-to-end performance. In tbis paper, we describe the design and implementation of a simple protocol, called the snoop protocol, that improves TCP performance in wireless networks. The protocol modifies network-layer software mainly at a base station and preserves end-to-end TCP semantics. The main idea of the protocol is to cache packets at the base station and perform local retransmissions across the wireless link. We have implemented the snoop protocol on a wireless testbed consisting of IBM ThinkPad laptops and i486 base stations communicating over an AT&T Wavelan. Our experiments show that it is significantly more robust at dealing with unreliable wireless links as compared to normal TCP; we have achieved throughput speedups of up to 20 times over regular TCP in our experiments with the protocol.},
	numpages     = 10
}
@inproceedings{sonuma,
	title        = {Scale-out NUMA},
	author       = {Novakovic, Stanko and Daglis, Alexandros and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
	year         = 2014,
	booktitle    = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
	location     = {Salt Lake City, Utah, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS '14},
	pages        = {3–18},
	doi          = {10.1145/2541940.2541965},
	isbn         = 9781450323055,
	url          = {https://doi.org/10.1145/2541940.2541965},
	abstract     = {Emerging datacenter applications operate on vast datasets that are kept in DRAM to minimize latency. The large number of servers needed to accommodate this massive memory footprint requires frequent server-to-server communication in applications such as key-value stores and graph-based applications that rely on large irregular data structures. The fine-grained nature of the accesses is a poor match to commodity networking technologies, including RDMA, which incur delays of 10-1000x over local DRAM operations. We introduce Scale-Out NUMA (soNUMA) -- an architecture, programming model, and communication protocol for low-latency, distributed in-memory processing. soNUMA layers an RDMA-inspired programming model directly on top of a NUMA memory fabric via a stateless messaging protocol. To facilitate interactions between the application, OS, and the fabric, soNUMA relies on the remote memory controller -- a new architecturally-exposed hardware block integrated into the node's local coherence hierarchy. Our results based on cycle-accurate full-system simulation show that soNUMA performs remote reads at latencies that are within 4x of local DRAM, can fully utilize the available memory bandwidth, and can issue up to 10M remote memory operations per second per core.},
	keywords     = {numa, system-on-chips, rmda},
	numpages     = 16
}
@inproceedings{star,
	title        = {{StaR}: Breaking the Scalability Limit for {RDMA}},
	author       = {Wang, Xizheng and Chen, Guo and Yin, Xijin and Dai, Huichen and Li, Bojie and Fu, Binzhang and Tan, Kun},
	year         = 2021,
	booktitle    = {Proceedings of the 29th IEEE International Conference on Network Protocols (ICNP)}
}
@inproceedings{storm,
	title        = {Storm: A Fast Transactional Dataplane for Remote Data Structures},
	author       = {Novakovic, Stanko and Shan, Yizhou and Kolli, Aasheesh and Cui, Michael and Zhang, Yiying and Eran, Haggai and Pismenny, Boris and Liss, Liran and Wei, Michael and Tsafrir, Dan and Aguilera, Marcos},
	year         = 2019,
	booktitle    = {Proceedings of the 12th ACM International Conference on Systems and Storage},
	location     = {Haifa, Israel},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SYSTOR '19},
	pages        = {97–108},
	doi          = {10.1145/3319647.3325827},
	isbn         = 9781450367493,
	url          = {https://doi.org/10.1145/3319647.3325827},
	abstract     = {RDMA technology enables a host to access the memory of a remote host without involving the remote CPU, improving the performance of distributed in-memory storage systems. Previous studies argued that RDMA suffers from scalability issues, because the NIC's limited resources are unable to simultaneously cache the state of all the concurrent network streams. These concerns led to various software-based proposals to reduce the size of this state by trading off performance.We revisit these proposals and show that they no longer apply when using newer RDMA NICs in rack-scale environments. In particular, we find that one-sided remote memory primitives lead to better performance as compared to the previously proposed unreliable datagram and kernel-based stacks. Based on this observation, we design and implement Storm, a transactional dataplane utilizing one-sided read and write-based RPC primitives. We show that Storm outperforms eRPC, FaRM, and LITE by 3.3x, 3.6x, and 17.1x, respectively, on an InfiniBand cluster with Mellanox ConnectX-4 NICs.},
	keywords     = {RPC, RDMA, data structures},
	numpages     = 12
}
@misc{supernic,
	title        = {Disaggregating and Consolidating Network Functionalities},
	author       = {Yizhou Shan and Will Lin and Ryan Kosta and Arvind Krishnamurthy and Yiying Zhang},
	year         = 2021,
	eprint       = {2109.07744},
	howpublished = {arXiv},
	primaryclass = {cs.DC}
}

@misc{p4,
	title = {P4 Language Specification},
	author = {P4 Language Consortium},
	year = 2017,
	howpublished = {\url{https://p4.org/p4-spec/docs/P4-16-v1.1.0-spec.html}}
}

@inproceedings{surf,
	title        = {SuRF: Practical Range Query Filtering with Fast Succinct Tries},
	author       = {Zhang, Huanchen and Lim, Hyeontaek and Leis, Viktor and Andersen, David G. and Kaminsky, Michael and Keeton, Kimberly and Pavlo, Andrew},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 International Conference on Management of Data},
	location     = {Houston, TX, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGMOD ’18},
	pages        = {323–336},
	doi          = {10.1145/3183713.3196931},
	isbn         = 9781450347037,
	url          = {https://doi.org/10.1145/3183713.3196931},
	keywords     = {surf, range filter, fast succinct tries, lsm-trees, succinct data structures},
	numpages     = 14
}
@inproceedings{switchml,
	title        = {Scaling Distributed Machine Learning with {In-Network} Aggregation},
	author       = {Amedeo Sapio and Marco Canini and Chen-Yu Ho and Jacob Nelson and Panos Kalnis and Changhoon Kim and Arvind Krishnamurthy and Masoud Moshref and Dan Ports and Peter Richtarik},
	year         = 2021,
	month        = {April},
	booktitle    = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
	publisher    = {USENIX Association},
	pages        = {785--808},
	isbn         = {978-1-939133-21-2},
	url          = {https://www.usenix.org/conference/nsdi21/presentation/sapio}
}
@inproceedings{tea,
	title        = {TEA: Enabling State-Intensive Network Functions on Programmable Switches},
	author       = {Kim, Daehyeok and Liu, Zaoxing and Zhu, Yibo and Kim, Changhoon and Lee, Jeongkeun and Sekar, Vyas and Seshan, Srinivasan},
	year         = 2020,
	booktitle    = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '20},
	pages        = {90–106},
	doi          = {10.1145/3387514.3405855},
	isbn         = 9781450379557,
	url          = {https://doi.org/10.1145/3387514.3405855},
	abstract     = {Programmable switches have been touted as an attractive alternative for deploying network functions (NFs) such as network address translators (NATs), load balancers, and firewalls. However, their limited memory capacity has been a major stumbling block that has stymied their adoption for supporting state-intensive NFs such as cloud-scale NATs and load balancers that maintain millions of flow-table entries. In this paper, we explore a new approach that leverages DRAM on servers available in typical NFV clusters. Our new system architecture, called TEA (Table Extension Architecture), provides a virtual table abstraction that allows NFs on programmable switches to look up large virtual tables built on external DRAM. Our approach enables switch ASICs to access external DRAM purely in the data plane without involving CPUs on servers. We address key design and implementation challenges in realizing this idea. We demonstrate its feasibility and practicality with our implementation on a Tofino-based programmable switch. Our evaluation shows that NFs built with TEA can look up table entries on external DRAM with low and predictable latency (1.8-2.2 μs) and the lookup throughput can be linearly scaled with additional servers (138 million lookups per seconds with 8 servers).},
	keywords     = {Programmable switches, Remote Direct Memory Access, Network Function Virtualization, Programmable networks, Data centers},
	numpages     = 17
}
@inproceedings{the-machine,
	title        = {Beyond Processor-centric Operating Systems},
	author       = {Paolo Faraboschi and Kimberly Keeton and Tim Marsland and Dejan Milojicic},
	year         = 2015,
	booktitle    = {15th Workshop on Hot Topics in Operating Systems (HotOS {XV})},
	publisher    = {{USENIX} Association},
	address      = {Kartause Ittingen, Switzerland},
	url          = {https://www.usenix.org/conference/hotos15/workshop-program/presentation/faraboschi}
}
@inproceedings{the-multikernel,
	title        = {The Multikernel: A new OS architecture for scalable multicore systems},
	author       = {Baumann, Andrew and Barham, Paul and Isaacs, Rebecca and Harris, Tim},
	year         = 2009,
	month        = {October},
	booktitle    = {22nd Symposium on Operating Systems Principles},
	publisher    = {Association for Computing Machinery, Inc.},
	url          = {https://www.microsoft.com/en-us/research/publication/the-multikernel-a-new-os-architecture-for-scalable-multicore-systems/},
	abstract     = {
		Commodity computer systems contain more and more processor cores and exhibit increasingly diverse architectural tradeoffs, including memory hierarchies, interconnects, instruction sets and variants, and IO configurations. Previous high-performance computing systems have scaled in specific cases, but the dynamic nature of modern client and server workloads, coupled with the impossibility of statically optimizing an OS for all workloads and hardware variants pose serious challenges for operating system structures.

		We argue that the challenge of future multicore hardware is best met by embracing the networked nature of the machine, rethinking OS architecture using ideas from distributed systems. We investigate a new OS structure, the multikernel, that treats the machine as a network of independent cores, assumes no inter-core sharing at the lowest level, and moves traditional OS functionality to a distributed system of processes that communicate via message-passing.

		We have implemented a multikernel OS to show that the approach is promising, and we describe how traditional scalability problems for operating systems (such as memory management) can be effectively recast using messages and can exploit insights from distributed systems and networking.  An evaluation of our prototype on multicore systems shows that, even on present-day machines, the performance of a multikernel is comparable with a conventional OS, and can scale better to support future hardware.
	},
	edition      = {22nd Symposium on Operating Systems Principles}
}
@misc{tofino2,
	title        = {Intel Tofino 2 P4 Programmability with More Bandwidth},
	year         = 2020,
	howpublished = {\url{https://www.intel.com/content/www/us/en/products/network-io/programmable-ethernet-switch/tofino-2-series/tofino-2.html}}
}
@inproceedings{trackfm,
	title        = {TrackFM: Far-out Compiler Support for a Far Memory World},
	author       = {Tauro, Brian R. and Suchy, Brian and Campanoni, Simone and Dinda, Peter and Hale, Kyle C.},
	year         = 2024,
	booktitle    = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
	location     = {, La Jolla, CA, USA,},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASPLOS '24},
	pages        = {401–419},
	doi          = {10.1145/3617232.3624856},
	isbn         = 9798400703720,
	url          = {https://doi.org/10.1145/3617232.3624856},
	abstract     = {Large memory workloads with favorable locality of reference can benefit by extending the memory hierarchy across machines. Systems that enable such far memory configurations can improve application performance and overall memory utilization in a cluster. There are two current alternatives for software-based far memory: kernel-based and library-based. Kernel-based approaches sacrifice performance to achieve programmer transparency, while library-based approaches sacrifice programmer transparency to achieve performance. We argue for a novel third approach, the compiler-based approach, which sacrifices neither performance nor programmer transparency. Modern compiler analysis and transformation techniques, combined with a suitable tightly-coupled runtime system, enable this approach. We describe the design, implementation, and evaluation of TrackFM, a new compiler-based far memory system. Through extensive benchmarking, we demonstrate that TrackFM outperforms kernel-based approaches by up to 2\texttimes{} while retaining their programmer transparency, and that TrackFM can perform similarly to a state-of-the-art library-based system (within 10\%). The application is merely recompiled to reap these benefits.},
	keywords     = {disaggregated memory, compilers, far memory},
	numpages     = 19
}
@inproceedings{understanding-pcie,
	title        = {Understanding PCIe Performance for End Host Networking},
	author       = {Neugebauer, Rolf and Antichi, Gianni and Zazo, Jos\'{e} Fernando and Audzevich, Yury and L\'{o}pez-Buedo, Sergio and Moore, Andrew W.},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
	location     = {Budapest, Hungary},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGCOMM '18},
	pages        = {327–341},
	doi          = {10.1145/3230543.3230560},
	isbn         = 9781450355674,
	url          = {https://doi.org/10.1145/3230543.3230560},
	abstract     = {In recent years, spurred on by the development and availability of programmable NICs, end hosts have increasingly become the enforcement point for core network functions such as load balancing, congestion control, and application specific network offloads. However, implementing custom designs on programmable NICs is not easy: many potential bottlenecks can impact performance.This paper focuses on the performance implication of PCIe, the de-facto I/O interconnect in contemporary servers, when interacting with the host architecture and device drivers. We present a theoretical model for PCIe and pcie-bench, an open-source suite, that allows developers to gain an accurate and deep understanding of the PCIe substrate. Using pcie-bench, we characterize the PCIe subsystem in modern servers. We highlight surprising differences in PCIe implementations, evaluate the undesirable impact of PCIe features such as IOMMUs, and show the practical limits for common network cards operating at 40Gb/s and beyond. Furthermore, through pcie-bench we gained insights which guided software and future hardware architectures for both commercial and research oriented network cards and DMA engines.},
	keywords     = {operating system, PCIe, reconfigurable hardware},
	numpages     = 15
}
@misc{upi,
	title        = {Intel Xeon Processor Scalable Family Technical Overview},
	author       = {David Mulnix},
	year         = 2017,
	url          = {https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html}
}
@inproceedings{when-computer,
	title        = {When Should The Network Be The Computer?},
	author       = {Ports, Dan R. K. and Nelson, Jacob},
	year         = 2019,
	booktitle    = {Proceedings of the Workshop on Hot Topics in Operating Systems},
	location     = {Bertinoro, Italy},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {HotOS '19},
	pages        = {209–215},
	doi          = {10.1145/3317550.3321439},
	isbn         = 9781450367271,
	url          = {https://doi.org/10.1145/3317550.3321439},
	abstract     = {Researchers have repurposed programmable network devices to place small amounts of application computation in the network, sometimes yielding orders-of-magnitude performance gains. At the same time, effectively using these devices requires careful use of limited resources and managing deployment challenges.This paper provides a framework for principled use of in-network processing. We provide a set of guidelines for building robust and deployable in-network primives, along with a taxonomy to help identify which applications can benefit from in-network processing and what types of devices they should use.},
	keywords     = {smart NICs, in-network computation, reconfigurable devices, programmable switches},
	numpages     = 7
}
@inproceedings{xrdma,
	title        = {X-RDMA: Effective RDMA Middleware in Large-scale Production Environments},
	author       = {Ma, Teng and Ma, Tao and Song, Zhuo and Li, Jingxuan and Chang, Huaixin and Chen, Kang and Jiang, Hai and Wu, Yongwei},
	year         = 2019,
	booktitle    = {2019 IEEE International Conference on Cluster Computing (CLUSTER)},
	volume       = {},
	number       = {},
	pages        = {1--12},
	doi          = {10.1109/CLUSTER.2019.8891004}
}
@misc{xxhash,
	title        = {xxHash Extremely fast hash algorithm},
	author       = {Yann Collet},
	year         = 2023,
	howpublished = {\url{https://xxhash.com/}}
}
@inproceedings{ycsb,
	title        = {Benchmarking Cloud Serving Systems with {YCSB}},
	author       = {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and Ramakrishnan, Raghu and Sears, Russell},
	year         = 2010,
	booktitle    = {Proceedings of the 1st ACM Symposium on Cloud Computing},
	location     = {Indianapolis, Indiana, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SoCC '10},
	pages        = {143–154},
	doi          = {10.1145/1807128.1807152},
	isbn         = 9781450300360,
	url          = {https://doi.org/10.1145/1807128.1807152},
	abstract     = {While the use of MapReduce systems (such as Hadoop) for large scale data analysis has been widely recognized and studied, we have recently seen an explosion in the number of systems developed for cloud data serving. These newer systems address "cloud OLTP" applications, though they typically do not support ACID transactions. Examples of systems proposed for cloud serving use include BigTable, PNUTS, Cassandra, HBase, Azure, CouchDB, SimpleDB, Voldemort, and many others. Further, they are being applied to a diverse range of applications that differ considerably from traditional (e.g., TPC-C like) serving workloads. The number of emerging cloud serving systems and the wide range of proposed applications, coupled with a lack of apples-to-apples performance comparisons, makes it difficult to understand the tradeoffs between systems and the workloads for which they are suited. We present the "Yahoo! Cloud Serving Benchmark" (YCSB) framework, with the goal of facilitating performance comparisons of the new generation of cloud data serving systems. We define a core set of benchmarks and report results for four widely used systems: Cassandra, HBase, Yahoo!'s PNUTS, and a simple sharded MySQL implementation. We also hope to foster the development of additional cloud benchmark suites that represent other classes of applications by making our benchmark tool available via open source. In this regard, a key feature of the YCSB framework/tool is that it is extensible--it supports easy definition of new workloads, in addition to making it easy to benchmark new systems.},
	keywords     = {cloud serving database, benchmarking},
	numpages     = 12
}
@inproceedings{zombieland,
	title        = {Welcome to {Zombieland}: Practical and Energy-efficient Memory Disaggregation in a Datacenter},
	author       = {Nitu, Vlad and Teabe, Boris and Tchana, Alain and Isci, Canturk and Hagimont, Daniel},
	year         = 2018,
	booktitle    = {Proceedings of the Thirteenth EuroSys Conference},
	location     = {Porto, Portugal},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {EuroSys '18},
	pages        = {16:1--16:12},
	doi          = {10.1145/3190508.3190537},
	isbn         = {978-1-4503-5584-1},
	url          = {http://doi.acm.org/10.1145/3190508.3190537},
	acmid        = 3190537,
	articleno    = 16,
	keywords     = {energy efficiency, memory disaggregation, virtualization},
	numpages     = 12
}

@inproceedings{eris,
author = {Li, Jialin and Michael, Ellis and Ports, Dan R. K.},
title = {Eris: Coordination-Free Consistent Transactions Using In-Network Concurrency Control},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132751},
doi = {10.1145/3132747.3132751},
abstract = {Distributed storage systems aim to provide strong consistency and isolation guarantees on an architecture that is partitioned across multiple shards for scalability and replicated for fault tolerance. Traditionally, achieving all of these goals has required an expensive combination of atomic commitment and replication protocols -- introducing extensive coordination overhead. Our system, Eris, takes a different approach. It moves a core piece of concurrency control functionality, which we term multi-sequencing, into the datacenter network itself. This network primitive takes on the responsibility for consistently ordering transactions, and a new lightweight transaction protocol ensures atomicity.The end result is that Eris avoids both replication and transaction coordination overhead: we show that it can process a large class of distributed transactions in a single round-trip from the client to the storage system without any explicit coordination between shards or replicas in the normal case. It provides atomicity, consistency, and fault tolerance with less than 10\% overhead -- achieving throughput 3.6-35x higher and latency 72-80\% lower than a conventional design on standard benchmarks.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {104–120},
numpages = {17},
keywords = {network multi-sequencing, in-network concurrency control, distributed transactions},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings {nopaxos,
author = {Jialin Li and Ellis Michael and Naveen Kr. Sharma and Adriana Szekeres and Dan R. K. Ports},
title = {Just Say {NO} to Paxos Overhead: Replacing Consensus with Network Ordering},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {467--483},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/li},
publisher = {USENIX Association},
month = nov
}

@inproceedings{fast-networks,
author = {Aguilera, Marcos K. and Amit, Nadav and Calciu, Irina and Deguillard, Xavier and Gandhi, Jayneel and Subrahmanyam, Pratap and Suresh, Lalith and Tati, Kiran and Venkatasubramanian, Rajesh and Wei, Michael},
title = {Remote memory in the age of fast networks},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131612},
doi = {10.1145/3127479.3131612},
abstract = {As the latency of the network approaches that of memory, it becomes increasingly attractive for applications to use remote memory---random-access memory at another computer that is accessed using the virtual memory subsystem. This is an old idea whose time has come, in the age of fast networks. To work effectively, remote memory must address many technical challenges. In this paper, we enumerate these challenges, discuss their feasibility, explain how some of them are addressed by recent work, and indicate other promising ways to tackle them. Some challenges remain as open problems, while others deserve more study. In this paper, we hope to provide a broad research agenda around this topic, by proposing more problems than solutions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {121–127},
numpages = {7},
location = {Santa Clara, California},
series = {SoCC '17}
}  

@inproceedings{ditto,
author = {Shen, Jiacheng and Zuo, Pengfei and Luo, Xuchuan and Su, Yuxin and Gu, Jiazhen and Feng, Hao and Zhou, Yangfan and Lyu, Michael R.},
title = {Ditto: An Elastic and Adaptive Memory-Disaggregated Caching System},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613144},
doi = {10.1145/3600006.3613144},
abstract = {In-memory caching systems are fundamental building blocks in cloud services. However, due to the coupled CPU and memory on monolithic servers, existing caching systems cannot elastically adjust resources in a resource-efficient and agile manner. To achieve better elasticity, we propose to port in-memory caching systems to the disaggregated memory (DM) architecture, where compute and memory resources are decoupled and can be allocated flexibly. However, constructing an elastic caching system on DM is challenging since accessing cached objects with CPU-bypass remote memory accesses hinders the execution of caching algorithms. Moreover, the elastic changes of compute and memory resources on DM affect the access patterns of cached data, compromising the hit rates of caching algorithms. We design Ditto, the first caching system on DM, to address these challenges. Ditto first proposes a client-centric caching framework to efficiently execute various caching algorithms in the compute pool of DM, relying only on remote memory accesses. Then, Ditto employs a distributed adaptive caching scheme that adaptively switches to the best-fit caching algorithm in real-time based on the performance of multiple caching algorithms to improve cache hit rates. Our experiments show that Ditto effectively adapts to the changing resources on DM and outperforms the state-of-the-art caching systems by up to 3.6\texttimes{} in real-world workloads and 9\texttimes{} in YCSB benchmarks.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {675–691},
numpages = {17},
keywords = {disaggregated memory, RDMA, key-value cache},
location = {<conf-loc>, <city>Koblenz</city>, <country>Germany</country>, </conf-loc>},
series = {SOSP '23}
}

@inproceedings{fairnic,
author = {Grant, Stewart and Yelam, Anil and Bland, Maxwell and Snoeren, Alex C.},
title = {SmartNIC Performance Isolation with FairNIC: Programmable Networking for the Cloud},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405895},
doi = {10.1145/3387514.3405895},
abstract = {Multiple vendors have recently released SmartNICs that provide both special-purpose accelerators and programmable processing cores that allow increasingly sophisticated packet processing tasks to be offloaded from general-purpose CPUs. Indeed, leading data-center operators have designed and deployed SmartNICs at scale to support both network virtualization and application-specific tasks. Unfortunately, cloud providers have not yet opened up the full power of these devices to tenants, as current runtimes do not provide adequate isolation between individual applications running on the SmartNICs themselves.We introduce FairNIC, a system to provide performance isolation between tenants utilizing the full capabilities of a commodity SoC SmartNIC. We implement FairNIC on Cavium LiquidIO 2360s and show that we are able to isolate not only typical packet processing, but also prevent MIPS-core cache pollution and fairly share access to fixed-function hardware accelerators. We use FairNIC to implement NIC-accelerated OVS and key/value store applications and show that they both can cohabitate on a single NIC using the same port, where the performance of each is unimpacted by other tenants. We argue that our results demonstrate the feasibility of sharing SmartNICs among virtual tenants, and motivate the development of appropriate security isolation mechanisms.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {681–693},
numpages = {13},
keywords = {performance isolation, cloud hosting, Network adapters},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings {treadmarks,
author = {Peter Keleher and Alan L. Cox and Sandhya Dwarkadas and Willy Zwaenepoel},
title = {Tread Marks: Distributed Shared Memory on Standard Workstations and Operating Systems},
booktitle = {USENIX Winter 1994 Technical Conference (USENIX Winter 1994 Technical Conference)},
year = {1994},
address = {San Francisco, CA},
url = {https://www.usenix.org/conference/usenix-winter-1994-technical-conference/tread-marks-distributed-shared-memory-standard},
publisher = {USENIX Association},
month = jan
}


@inproceedings{cxl-demyst, series={MICRO ’23},
   title={Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices},
   url={http://dx.doi.org/10.1145/3613424.3614256},
   DOI={10.1145/3613424.3614256},
   booktitle={56th Annual IEEE/ACM International Symposium on Microarchitecture},
   publisher={ACM},
   author={Sun, Yan and Yuan, Yifan and Yu, Zeduo and Kuper, Reese and Song, Chihun and Huang, Jinghan and Ji, Houxiang and Agarwal, Siddharth and Lou, Jiaqi and Jeong, Ipoom and Wang, Ren and Ahn, Jung Ho and Xu, Tianyin and Kim, Nam Sung},
   year={2023},
   month=oct, collection={MICRO ’23} }

@inproceedings{tpp,
author = {Maruf, Hasan Al and Wang, Hao and Dhanotia, Abhishek and Weiner, Johannes and Agarwal, Niket and Bhattacharya, Pallab and Petersen, Chris and Chowdhury, Mosharaf and Kanaujia, Shobhit and Chauhan, Prakash},
title = {TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582063},
doi = {10.1145/3582016.3582063},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {742–755},
numpages = {14},
keywords = {CXL-Memory, Datacenters, Heterogeneous System, Memory Management, Operating Systems, Tiered-Memory},
location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {ASPLOS 2023}
}

@inproceedings{pond,
author = {Li, Huaicheng and Berger, Daniel S. and Novakovic, Stanko and Hsu, Lisa and Ernst, Dan and Zardoshti, Pantea and Shah, Monish and Rajadnya, Samir and Lee, Scott and Agarwal, Ishwar and Hill, Mark D. and Fontoura, Marcus and Bianchini, Ricardo},
title = {Pond: CXL-Based Memory Pooling Systems for Cloud Platforms},
organization = {ACM},
booktitle = {Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
year = {2023},
month = {March},
}